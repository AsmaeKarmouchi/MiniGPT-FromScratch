{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AsmaeKarmouchi/MiniGPT-FromScratch/blob/main/Mini_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build a small GPT**"
      ],
      "metadata": {
        "id": "iuVjqf2aMeby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objectif du notebook : expliquer et impl√©menter pas √† pas un mini-mod√®le de langage (bigram -> introduction aux transformers) en PyTorch, en utilisant le dataset Tiny Shakespeare (1 MB)."
      ],
      "metadata": {
        "id": "D8gdLZ06Mnvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le TRANSFORMER fait tout le travail lourd sous le capot\n",
        "-  Paper r√©f√©rence : Attention Is All You Need (Vaswani et al. 2017)\n",
        "-  Notre objectif ici : entra√Æner un mod√®le de langage bas√© sur un TRANSFORMER."
      ],
      "metadata": {
        "id": "tdzL5-G3RZTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 0 ‚Äî Download the dataset (Tiny Shakespeare)**"
      ],
      "metadata": {
        "id": "IxLJpIypNWR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget -q https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O input.txt\n",
        "# -q pour mode silencieux, -O pour nommer le fichier local\n",
        "!ls -lh input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK_3E4zUMm0q",
        "outputId": "51b03402-3aec-4e45-a8b7-bbc596e6c411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 1.1M Dec  7 19:37 input.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1 ‚Äî Imports and Hyperparameters**"
      ],
      "metadata": {
        "id": "avJwEHhTNeJc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmJ6JiXJY2ZV"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import random\n",
        "\n",
        "\n",
        "# Hyperparam√®tres (exp√©rimentaux: tu peux les ajuster)\n",
        "batch_size = 32 # nombre d'exemples trait√©s en parall√®le\n",
        "block_size = 8 # longueur du contexte (nombre de tokens visibles pour pr√©dire le suivant)\n",
        "max_iters = 3000 # nombre d'it√©rations d'entra√Ænement\n",
        "eval_interval = 300 # fr√©quence d'√©valuation (en it√©rations)\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "\n",
        "\n",
        "# Reproductibilit√©\n",
        "torch.manual_seed(1337)\n",
        "random.seed(1337)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  torch : pour les tenseurs (matrices) et calculs GPU\n",
        "\n",
        "* nn : pour cr√©er des r√©seaux neuronaux\n",
        "\n",
        "* F : pour utiliser des fonctions comme softmax, cross_entropy‚Ä¶\n",
        "\n"
      ],
      "metadata": {
        "id": "9SauWS55OYju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Param√®tre        | Signification                                                 |\n",
        "| ---------------- | ------------------------------------------------------------- |\n",
        "| batch_size = 32  | Nombre d‚Äôexemples trait√©s en m√™me temps                       |\n",
        "| block_size = 8   | Longueur maximale du contexte (on regarde 8 caract√®res avant) |\n",
        "| max_iters = 3000 | Nombre d‚Äôit√©rations d'apprentissage                           |\n",
        "| learning_rate    | Taux d‚Äôapprentissage                                          |\n",
        "| device           | GPU si disponible sinon CPU                                   |\n"
      ],
      "metadata": {
        "id": "BxtZV_YwOi31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part2 Loading text and building vocabulary**"
      ],
      "metadata": {
        "id": "jxntVIm0NnJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lire le fichier\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "print(f\"Taille du texte: {len(text)} caract√®res\")\n",
        "print(text[:500]) # afficher les 500 premiers caract√®res pour se faire une id√©e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yaWxquBM01k",
        "outputId": "7ce8d115-fb43-48f9-ae00-4729fcddb646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taille du texte: 1115394 caract√®res\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulaire (caract√®res uniques)\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"Vocab size: {vocab_size}\")\n",
        "print(chars)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlsNuiqjRmqy",
        "outputId": "68e1fe89-cc50-4687-cf90-67dd49783332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 65\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "set(text) ‚Üí ensemble des caract√®res uniques\n",
        "\n",
        "list + sorted ‚Üí pour avoir un vocabulaire ordonn√©\n",
        "\n",
        "vocab_size ‚Üí nombre de symboles possibles que le mod√®le peut voir ou produire"
      ],
      "metadata": {
        "id": "v_sWVVlaOxpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mappings\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n"
      ],
      "metadata": {
        "id": "VomsDwkLM20g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization : Qu‚Äôest-ce qu‚Äôun Token ?\n",
        "\n",
        "Un token est la mani√®re num√©rique de repr√©senter le texte.\n",
        "\n",
        "Ici :\n",
        "\n",
        "On ne prend pas des mots, mais des caract√®res\n",
        "\n",
        "On convertit chaque caract√®re en entier"
      ],
      "metadata": {
        "id": "vPts4Sl7Rwow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode / Decode helpers\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n"
      ],
      "metadata": {
        "id": "JQ5wwpiBM4Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le mod√®le ne comprend que des nombres\n",
        "\n",
        "On convertit donc chaque caract√®re en entier."
      ],
      "metadata": {
        "id": "Hl-2LgFjO7el"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Google utilise SentencePiece (sous-mots)\n",
        "- OpenAI utilise tiktoken\n",
        "Nous utilisons ici une version minimale, suffisante pour comprendre GPT."
      ],
      "metadata": {
        "id": "rfyNxcixR1nE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tester encode/decode\n",
        "sample = \"To be, or not to be\"\n",
        "encoded = encode(sample)\n",
        "decoded = decode(encoded)\n",
        "print(sample)\n",
        "print(encoded)\n",
        "print(decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9i90tvCM6CK",
        "outputId": "4a057b25-ae48-432c-8eac-49a53fcd446f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be, or not to be\n",
            "[32, 53, 1, 40, 43, 6, 1, 53, 56, 1, 52, 53, 58, 1, 58, 53, 1, 40, 43]\n",
            "To be, or not to be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3 ‚Äî Split train/val & conversion to tensors**"
      ],
      "metadata": {
        "id": "qioTRpTwNwWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer tout le texte en tenseur d'indices\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "\n",
        "# Split train/val\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(f\"Train tokens: {len(train_data)}, Val tokens: {len(val_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_JNkIdIM91e",
        "outputId": "bcaae59f-f1c4-46ed-e7ce-1aa340656254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train tokens: 1003854, Val tokens: 111540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On garde 10% des donn√©es cach√©es pour mesurer l‚Äôoverfitting\n"
      ],
      "metadata": {
        "id": "QfcPu1NrPJC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Partie 4 ‚Äî DataLoader simple : get_batch**"
      ],
      "metadata": {
        "id": "Ypgwaq2rN2sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "ofw9WJq0NACF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Concept      | R√¥le                              |\n",
        "| ------------ | --------------------------------- |\n",
        "| `block_size` | longueur maximale du contexte     |\n",
        "| `batch_size` | combien de s√©quences en parall√®le |\n"
      ],
      "metadata": {
        "id": "TZ0MW86dSGha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour entra√Æner le mod√®le, on r√©cup√®re 8 caract√®res (x) et on demande au mod√®le de pr√©dire le 9√®me (y).\n",
        "x = s√©quence de caract√®res\n",
        "\n",
        "y = m√™mes caract√®res mais d√©cal√©s d‚Äôun cran"
      ],
      "metadata": {
        "id": "NUtedVR7Pd1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tester la fonction\n",
        "xb, yb = get_batch('train')\n",
        "print('xb shape:', xb.shape) # (B, T)\n",
        "print('yb shape:', yb.shape) # (B, T)\n",
        "print('Premier exemple (tokens):', xb[0].tolist())\n",
        "print('Converti en texte :', decode(xb[0].tolist()))\n",
        "print('Targets (texte):', decode(yb[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGhVnGiNPWbC",
        "outputId": "ac91f953-6d75-4587-d433-001f7f582234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xb shape: torch.Size([32, 8])\n",
            "yb shape: torch.Size([32, 8])\n",
            "Premier exemple (tokens): [24, 43, 58, 5, 57, 1, 46, 43]\n",
            "Converti en texte : Let's he\n",
            "Targets (texte): et's hea\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 5 ‚Äî Evaluation function (estimate_loss)**"
      ],
      "metadata": {
        "id": "195jdLa9N9BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "_OKc1Ij4NGCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On mesure l‚Äôerreur sans entra√Æner le mod√®le.\n",
        "\n",
        "Pour √©valuer train_loss et val_loss.\n",
        "pas de gradient (gain m√©moire)\n",
        "\n",
        "deux pertes : train & val\n",
        " permet de voir l‚Äôoverfitting"
      ],
      "metadata": {
        "id": "JXRH-HY1PmQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 6 ‚Äî The model: BigramLanguageModel**"
      ],
      "metadata": {
        "id": "1QAUNndsOEE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Pas d‚Äôattention, pas de m√©moire, mod√®le extr√™mement simple.\n",
        "- bigram model pr√©dit le prochain caract√®re uniquement √† partir du caract√®re actuel, pas du contexte entier\n",
        "\n",
        "- Chaque entier ‚Üí vecteur de dimension vocab_size"
      ],
      "metadata": {
        "id": "1YFcp4NZSTm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "#logits = les probabilit√©s non normalis√©es pour le prochain caract√®re\n",
        "#Puis on calcule la loss cross entropy si targets est fourni.\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "ARxzE0sPNIXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Embedding cr√©e une table qui associe chaque caract√®re √† un vecteur (ici un vecteur de taille vocab_size).\n",
        "Le mod√®le apprend :\n",
        "Si je vois ce caract√®re ‚Üí quel est le prochain caract√®re probable ?"
      ],
      "metadata": {
        "id": "wb3ENkDiQd2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- logits = les probabilit√©s non normalis√©es pour le prochain caract√®re\n",
        "- Puis on calcule la loss cross entropy si targets est fourni."
      ],
      "metadata": {
        "id": "g2O-eRVmQzOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "generate = g√©n√©ration de texte\n",
        "\n",
        "Le mod√®le :\n",
        "\n",
        "- Regarde le dernier caract√®re\n",
        "\n",
        "- Pr√©dit le prochain\n",
        "\n",
        "- L‚Äôajoute √† la s√©quence\n",
        "\n",
        "- Recommence"
      ],
      "metadata": {
        "id": "3wKP1vf_Q80n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| √âtape          | Shape     |\n",
        "| -------------- | --------- |\n",
        "| idx (indices)  | (B, T)    |\n",
        "| embeddings     | (B, T, C) |\n",
        "| C = vocab_size |           |\n"
      ],
      "metadata": {
        "id": "dtxPSYGQSa_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si targets ‚â† None ‚Üí calcul de la loss\n",
        "\n",
        "On transforme :\n",
        "\n",
        "(B, T, C) ‚Üí (B*T, C)\n",
        "\n",
        "(B, T) ‚Üí (B*T)"
      ],
      "metadata": {
        "id": "gMlqyOTBSekl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 7 ‚Äî Initializing the model, optimizing, and training**"
      ],
      "metadata": {
        "id": "TsL_-L9GOLLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Mode  | Dropout ? | But                 |\n",
        "| ----- | --------- | ------------------- |\n",
        "| train | Oui       | Apprentissage       |\n",
        "| eval  | Non       | Pr√©dictions stables |\n"
      ],
      "metadata": {
        "id": "51XtlwIATM9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel(vocab_size)\n",
        "model = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_H0T971KNKY7",
        "outputId": "69bc1c82-cf64-4c8d-b962-bca695112393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.7676, val loss 4.7677\n",
            "step 300: train loss 2.8381, val loss 2.8538\n",
            "step 600: train loss 2.5531, val loss 2.5811\n",
            "step 900: train loss 2.4967, val loss 2.5151\n",
            "step 1200: train loss 2.4878, val loss 2.5086\n",
            "step 1500: train loss 2.4677, val loss 2.4946\n",
            "step 1800: train loss 2.4695, val loss 2.4961\n",
            "step 2100: train loss 2.4707, val loss 2.4871\n",
            "step 2400: train loss 2.4645, val loss 2.4892\n",
            "step 2700: train loss 2.4736, val loss 2.4920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On utilise un optimiseur pour modifier les poids du mod√®le.\n",
        "\n",
        "√Ä chaque it√©ration :\n",
        "\n",
        "- On r√©cup√®re un batch\n",
        "\n",
        "- On calcule la perte\n",
        "\n",
        "- On fait loss.backward()\n",
        "\n",
        "- On met √† jour les poids"
      ],
      "metadata": {
        "id": "m6HnRy_iRIWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pourquoi AdamW (et pas juste Adam) ?\n",
        "\n",
        "‚úî weight decay d√©corr√©l√© du gradient\n",
        "‚úî meilleure g√©n√©ralisation\n",
        "‚úî standard des Transformers (ICLR 2019)"
      ],
      "metadata": {
        "id": "Ufs2Pn77S3rn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 8 ‚Äî Generating Text**"
      ],
      "metadata": {
        "id": "qlGCD0f2OSiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "generated = model.generate(context, max_new_tokens=500)[0].tolist()\n",
        "print(decode(generated))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SITvLltwNNTO",
        "outputId": "ce3b885b-a309-40ca-db68-3071717aeb45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "CEThik brid owindakis b, bth\n",
            "\n",
            "HAPen bobe d e.\n",
            "S:\n",
            "O:\n",
            "IS:\n",
            "Falatanss:\n",
            "Wanthar u qur, t.\n",
            "War dilasoate awice my.\n",
            "\n",
            "Hastarom oroup\n",
            "Yowhthetof isth ble mil ndill, ath iree sengmin lat Heriliovets, and Win nghir.\n",
            "Swanousel lind me l.\n",
            "HAshe ce hiry:\n",
            "Supr aisspllw y.\n",
            "Hentofu n Boopetelaves\n",
            "MPOFry wod mothakleo Windo whthCoribyo the m dourive we higend t so mower; te\n",
            "\n",
            "AN ad nterupt f s ar igr t m:\n",
            "\n",
            "Thin maleronth,\n",
            "Mad\n",
            "RD:\n",
            "\n",
            "WISo myrangoube!\n",
            "KENob&isarardsal thes ghesthinin couk ay aney Iry ts I fr y ce.\n",
            "J\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "exp: On commence par le caract√®re index = 0, et on g√©n√®re 500 caract√®res."
      ],
      "metadata": {
        "id": "KX0o3pvbROr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Texte ‚Üí Encodage ‚Üí S√©quences (x,y) ‚Üí Mod√®le Bigram ‚Üí Entra√Ænement ‚Üí G√©n√©ration\n"
      ],
      "metadata": {
        "id": "HP67unXjRRfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principe du Self-Attention\n",
        "\n",
        "Dans un mod√®le Transformer (GPT, BERT‚Ä¶), chaque token peut **¬´ voir ¬ª tous les autres tokens de la s√©quence** gr√¢ce au self-attention.\n",
        "\n",
        "---\n",
        "\n",
        "## Les trois vecteurs fondamentaux : Q, K, V\n",
        "\n",
        "Pour chaque token dans la s√©quence :\n",
        "\n",
        "- **Q (Query)** ‚Üí ¬´ je cherche √† quoi pr√™ter attention ¬ª  \n",
        "- **K (Key)** ‚Üí ¬´ voici ma cl√© d‚Äôinformation ¬ª  \n",
        "- **V (Value)** ‚Üí ¬´ voici la valeur √† communiquer ¬ª\n",
        "\n",
        "---\n",
        "\n",
        "## Communication entre tokens\n",
        "\n",
        "Chaque token ¬´ regarde ¬ª tous les autres tokens dans la s√©quence (ou seulement les pr√©c√©dents si causal / GPT) :\n",
        "\n",
        "- **GPT (causal attention)** :  \n",
        "  Masque la future information ‚Üí token t voit seulement 0‚Ä¶t  \n",
        "  ‚Üí Aucun tricheur sur le futur !\n",
        "\n",
        "- **BERT (bidirectional)** :  \n",
        "  Peut regarder tout le contexte 0‚Ä¶T\n",
        "\n",
        "---\n",
        "\n",
        "## Calcul de l‚Äôattention\n",
        "\n",
        "On calcule l‚Äôattention via la formule :\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{Q K^\\top}{\\sqrt{d_k}} \\right) V\n",
        "$$\n",
        "\n",
        "\\text{o√π :}\n",
        "$$\n",
        "Q \\in \\mathbb{R}^{n \\times d_k}, \\quad\n",
        "K \\in \\mathbb{R}^{n \\times d_k}, \\quad\n",
        "V \\in \\mathbb{R}^{n \\times d_v}\n",
        "$$\n",
        "\n",
        "$$\n",
        "d_k \\text{ : dimension des keys (facteur de scaling)}, \\quad\n",
        "n \\text{ : nombre de tokens dans la s√©quence}\n",
        "$$\n",
        "\n",
        "1. Q = Query\n",
        "\n",
        "¬´ Je cherche √† quoi pr√™ter attention ¬ª\n",
        "\n",
        "Repr√©sente le token courant qui veut regarder les autres tokens.\n",
        "\n",
        "2. K = Key\n",
        "\n",
        "¬´ Voici ma cl√© d‚Äôinformation ¬ª\n",
        "\n",
        "Chaque token poss√®de une cl√© qui indique son identit√© / contenu.\n",
        "\n",
        "3. V = Value\n",
        "\n",
        "¬´ Voici ma valeur √† communiquer ¬ª\n",
        "\n",
        "Contient l‚Äôinformation r√©elle que le token va partager avec les autres.\n",
        "\n",
        "---\n",
        "\n",
        "## Points cl√©s\n",
        "\n",
        "| Vecteur | R√¥le |\n",
        "|---------|------|\n",
        "| Q       | qui √©coute (¬´ Je veux √©couter ¬ª) |\n",
        "| K       | identit√© des tokens (¬´ Voici mon message ¬ª) |\n",
        "| V       | contenu r√©el √† partager (¬´ Voil√† ce que je peux partager ¬ª) |\n",
        "\n",
        "> Self-attention = combinaison intelligente de V en fonction de Q et K\n",
        "\n",
        "---\n",
        "\n",
        "## Truc math√©matique cl√©\n",
        "\n",
        "1. On projette chaque token en **Q, K, V**  \n",
        "2. Produit scalaire \\(Q \\cdot K^T\\) ‚Üí scores d‚Äôattention  \n",
        "3. softmax ‚Üí poids normalis√©s  \n",
        "4. Poids appliqu√©s √† V ‚Üí nouveau vecteur pour le token  \n",
        "\n",
        "**R√©sultat :** chaque token int√®gre l‚Äôinformation de tous les tokens qu‚Äôil peut voir.\n"
      ],
      "metadata": {
        "id": "Gtmv7e5Mpg12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#xbow[b, t] : moyenne des embeddings des tokens de 0 √† t\n",
        "import torch\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "B, T, C = 4, 8, 2\n",
        "x = torch.randn(B, T, C)\n",
        "print(\"x.shape:\", x.shape)  # torch.Size([4, 8, 2])\n",
        "\n",
        "# Cr√©er un tenseur vide pour le \"bag-of-words\" cumulatif\n",
        "xbow = torch.zeros((B, T, C))\n",
        "\n",
        "# Boucle pour calculer la moyenne cumulative\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b, :t+1]           # tous les tokens jusqu'√† t inclus\n",
        "        xbow[b, t] = torch.mean(xprev, dim=0)  # moyenne sur la dimension du temps\n",
        "\n",
        "print(\"xbow.shape:\", xbow.shape)  # torch.Size([4, 8, 2])\n",
        "print(xbow)\n",
        "print(x[0])\n",
        "print(xbow[0]) #\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9zekerhphTJ",
        "outputId": "875a083c-1b2a-4077-dbc0-fd8bf37f2827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.shape: torch.Size([4, 8, 2])\n",
            "xbow.shape: torch.Size([4, 8, 2])\n",
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.0894, -0.4926],\n",
            "         [ 0.1490, -0.3199],\n",
            "         [ 0.3504, -0.2238],\n",
            "         [ 0.3525,  0.0545],\n",
            "         [ 0.0688, -0.0396],\n",
            "         [ 0.0927, -0.0682],\n",
            "         [-0.0341,  0.1332]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.8173,  0.4127],\n",
            "         [-0.1342,  0.4395],\n",
            "         [ 0.2711,  0.4774],\n",
            "         [ 0.2421,  0.0694],\n",
            "         [ 0.0084,  0.0020],\n",
            "         [ 0.0712, -0.1128],\n",
            "         [ 0.2527,  0.2149]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 0.1735, -0.0649],\n",
            "         [ 0.1685,  0.3348],\n",
            "         [-0.1621,  0.1765],\n",
            "         [-0.2312, -0.0436],\n",
            "         [-0.1015, -0.2855],\n",
            "         [-0.2593, -0.1630],\n",
            "         [-0.3015, -0.2293]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.4985, -0.5395],\n",
            "         [ 0.4954,  0.3420],\n",
            "         [ 1.0623, -0.1802],\n",
            "         [ 1.1401, -0.4462],\n",
            "         [ 1.0870, -0.4071],\n",
            "         [ 1.0430, -0.1299],\n",
            "         [ 1.1138, -0.1641]]])\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398],\n",
            "        [-0.9211,  1.5433]])\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using matrix multiplication\n",
        "#exemple\n",
        "# how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOpJWLiks757",
        "outputId": "62925a08-edfc-463c-ef07-54b7b21a78d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "vectoriser le calcul du bag-of-words cumulatif avec une matrice de poids triangulaire inf√©rieure (torch.tril)."
      ],
      "metadata": {
        "id": "b2_sYxD_xy6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wei = torch.tril(torch.ones(T, T))\n",
        "# Diviser chaque ligne par le nombre de 1 cumul√©s\n",
        "counts = wei.sum(dim=1, keepdim=True)\n",
        "wei = wei / counts  # shape (T, T)\n",
        "\n",
        "# On ajoute une dimension batch pour que matmul fonctionne correctement\n",
        "wei = wei.unsqueeze(0)  # shape (1, T, T)\n",
        "xbow2 = torch.matmul(wei, x)  # shape (B, T, C), broadcast sur B\n",
        "\n",
        "# V√©rification\n",
        "torch.allclose(xbow, xbow2 , atol=1e-6, rtol=1e-5)\n",
        "print(torch.allclose(xbow, xbow2 , atol=1e-6, rtol=1e-5)\n",
        ")\n",
        "print(\"xbow2.shape:\", xbow2.shape)  # torch.Size([4, 8, 2])\n",
        "print(\"xbow.shape:\", xbow.shape)  # torch.Size([4, 8, 2])\n",
        "print(xbow2)\n",
        "print(xbow)\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4duQkfI2wwyM",
        "outputId": "732b5ad8-bea4-472f-ff00-43a1ee285a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "xbow2.shape: torch.Size([4, 8, 2])\n",
            "xbow.shape: torch.Size([4, 8, 2])\n",
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.0894, -0.4926],\n",
            "         [ 0.1490, -0.3199],\n",
            "         [ 0.3504, -0.2238],\n",
            "         [ 0.3525,  0.0545],\n",
            "         [ 0.0688, -0.0396],\n",
            "         [ 0.0927, -0.0682],\n",
            "         [-0.0341,  0.1332]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.8173,  0.4127],\n",
            "         [-0.1342,  0.4395],\n",
            "         [ 0.2711,  0.4774],\n",
            "         [ 0.2421,  0.0694],\n",
            "         [ 0.0084,  0.0020],\n",
            "         [ 0.0712, -0.1128],\n",
            "         [ 0.2527,  0.2149]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 0.1735, -0.0649],\n",
            "         [ 0.1685,  0.3348],\n",
            "         [-0.1621,  0.1765],\n",
            "         [-0.2312, -0.0436],\n",
            "         [-0.1015, -0.2855],\n",
            "         [-0.2593, -0.1630],\n",
            "         [-0.3015, -0.2293]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.4985, -0.5395],\n",
            "         [ 0.4954,  0.3420],\n",
            "         [ 1.0623, -0.1802],\n",
            "         [ 1.1401, -0.4462],\n",
            "         [ 1.0870, -0.4071],\n",
            "         [ 1.0430, -0.1299],\n",
            "         [ 1.1138, -0.1641]]])\n",
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.0894, -0.4926],\n",
            "         [ 0.1490, -0.3199],\n",
            "         [ 0.3504, -0.2238],\n",
            "         [ 0.3525,  0.0545],\n",
            "         [ 0.0688, -0.0396],\n",
            "         [ 0.0927, -0.0682],\n",
            "         [-0.0341,  0.1332]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.8173,  0.4127],\n",
            "         [-0.1342,  0.4395],\n",
            "         [ 0.2711,  0.4774],\n",
            "         [ 0.2421,  0.0694],\n",
            "         [ 0.0084,  0.0020],\n",
            "         [ 0.0712, -0.1128],\n",
            "         [ 0.2527,  0.2149]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 0.1735, -0.0649],\n",
            "         [ 0.1685,  0.3348],\n",
            "         [-0.1621,  0.1765],\n",
            "         [-0.2312, -0.0436],\n",
            "         [-0.1015, -0.2855],\n",
            "         [-0.2593, -0.1630],\n",
            "         [-0.3015, -0.2293]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.4985, -0.5395],\n",
            "         [ 0.4954,  0.3420],\n",
            "         [ 1.0623, -0.1802],\n",
            "         [ 1.1401, -0.4462],\n",
            "         [ 1.0870, -0.4071],\n",
            "         [ 1.0430, -0.1299],\n",
            "         [ 1.1138, -0.1641]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "#simuler un mask causal avec softmax pour reproduire le bag-of-words cumulatif.\n",
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3 , atol=1e-6, rtol=1e-5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi0ZSo5hyA7F",
        "outputId": "b74a6ae9-7a95-4269-b919-45774436d533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Id√©e g√©n√©rale du masque dans le self-attention causal (GPT)\n",
        "\n",
        "Dans un mod√®le de type GPT, chaque mot/token ne doit PAS utiliser des informations qui viennent du FUTUR.\n",
        "Donc, quand on calcule quelles parties de la phrase un token doit ¬´ √©couter ¬ª, on autorise seulement les tokens du pass√© et le token courant.\n",
        "\n",
        "Pour appliquer cette r√®gle dans le r√©seau :\n",
        "\n",
        "On cr√©e une matrice qui bloque toutes les positions futures.\n",
        "\n",
        "On remplace les positions interdites par ‚àí‚àû avant softmax.\n",
        "\n",
        "Le softmax transforme ces ‚àí‚àû en probabilit√© 0, donc le mod√®le ne peut pas utiliser ces tokens."
      ],
      "metadata": {
        "id": "vMIJX2yeQeSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les tokens futurs ont poids = 0, donc aucune information ne passe depuis le futur ‚Üí respect du causality !"
      ],
      "metadata": {
        "id": "I1Sj8GrpQTFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}, \\quad i = 1, \\dots, n\n",
        "$$\n"
      ],
      "metadata": {
        "id": "IumfP1qsGCtt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chaque valeur\n",
        "ùëß\n",
        "ùëñ\n",
        "\n",
        " devient un nombre entre 0 et 1\n",
        "La somme de toutes les valeurs = 1 ‚Üí c‚Äôest une distribution de probabilit√©"
      ],
      "metadata": {
        "id": "UzKID6dcGOet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "OkNOBdzqRR6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "Gr√¢ce √†  Q,K,V\n",
        "Certains tokens auront plus d‚Äôaffinit√© (plus de poids) : ils sont plus pertinents pour comprendre le token courant.\n",
        "\n",
        "D‚Äôautres auront moins d‚Äôaffinit√© (moins de poids) : ils sont moins utiles.\n",
        "\n",
        "Ces affinit√©s sont d√©pendantes des donn√©es ‚Üí elles changent selon le texte.\n"
      ],
      "metadata": {
        "id": "gaWCryBxRGT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "GAAInZhfS0Xn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Le token embedding : on convertit un mot en vecteur\n",
        "| Mot   | Embedding (exemple)     |\n",
        "| ----- | ----------------------- |\n",
        "| \"cat\" | ([0.2, -0.5, 0.3, 0.7]) |\n",
        "| \"dog\" | ([0.1, -0.2, 0.8, 0.4]) |\n",
        "\n",
        "\n",
        " Mais ce vecteur n‚Äôest pas encore une pr√©diction, c‚Äôest juste une repr√©sentation !\n",
        "Le but des logits\n",
        "\n",
        "√Ä la fin, on doit pr√©dire le prochain token.\n",
        "Donc on doit produire un score (logit) pour chaque token du vocabulaire.\n",
        "Si vocabulaire = 50 000 mots\n",
        "On veut un vecteur de dimension 50 000 !\n",
        "C‚Äôest exactement ce que fait une couche lin√©aire : nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "\n",
        " Donc :\n",
        "Nous devons transformer l‚Äôembedding (ex. 4 dimensions) ‚Üí logits (50 000 dimensions)\n",
        "Elle effectue :\n",
        "\n",
        "\n",
        "logits=W‚ãÖembedding+b\n",
        "\n",
        "W = matrice de poids\n",
        "\n",
        "b = biais\n",
        "\n",
        "embedding = vecteur repr√©sentant le mot"
      ],
      "metadata": {
        "id": "tICFTq5jS-Xi"
      }
    }
  ]
}