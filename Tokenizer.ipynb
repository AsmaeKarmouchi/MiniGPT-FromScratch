{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdYbMBYkkwT5X7iekh/i78",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AsmaeKarmouchi/MiniGPT-FromScratch/blob/main/Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build the GPT Tokenizer\n"
      ],
      "metadata": {
        "id": "EfRUK4i-mxYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# R√©sum√© : R√¥le et probl√®mes du Tokenizer dans les LLM\n",
        "\n",
        "## 1. Qu‚Äôest-ce qu‚Äôun Tokenizer ?\n",
        "Le Tokenizer est une √©tape s√©par√©e et indispensable des mod√®les de langage (LLMs).  \n",
        "Il sert √† convertir :\n",
        "- `encode()` : texte ‚Üí tokens (entiers)\n",
        "- `decode()` : tokens ‚Üí texte\n",
        "\n",
        "Les mod√®les comme GPT **ne travaillent jamais directement avec du texte**, mais uniquement avec des tokens.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Un syst√®me ind√©pendant du mod√®le\n",
        "Le tokenizer :\n",
        "- a son **propre jeu d‚Äôentra√Ænement**,\n",
        "- utilise **son propre algorithme** (souvent Byte Pair Encoding ‚Äî BPE),\n",
        "- est **entra√Æn√© s√©par√©ment**,\n",
        "- n‚Äôest pas un r√©seau neuronal.\n",
        "\n",
        "Une fois entra√Æn√©, il ne change plus.  \n",
        "‚Üí Le LLM doit s‚Äôadapter aux tokens fournis, m√™me s‚Äôils sont imparfaits.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Comment fonctionne BPE ?\n",
        "Le Byte Pair Encoding construit un vocabulaire de sous-mots :\n",
        "1. commence avec des caract√®res,\n",
        "2. fusionne les paires les plus fr√©quentes,\n",
        "3. continue jusqu‚Äô√† obtenir des milliers de morceaux optimaux.\n",
        "\n",
        "Exemples :\n",
        "- \"playing\" ‚Üí \"play\" + \"ing\"\n",
        "- \"unbelievable\" ‚Üí \"un\" + \"bel\" + \"ievable\"\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Pourquoi beaucoup de probl√®mes des LLM viennent de la tokenisation ?\n",
        "Parce que le mod√®le :\n",
        "- **ne voit pas des mots**,  \n",
        "- mais seulement des **tokens**.\n",
        "\n",
        "Cons√©quences :\n",
        "- mots simples ‚Üí plusieurs tokens,\n",
        "- mots rares ‚Üí tokenisation incoh√©rente,\n",
        "- accents, emojis, espaces ‚Üí comportements √©tranges,\n",
        "- erreurs d'interpr√©tation ‚Üí caus√©es par la fa√ßon dont le texte est d√©coup√©.\n",
        "\n",
        "Le mod√®le est limit√© par les choix faits lors de la cr√©ation du tokenizer.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Pourquoi vouloir supprimer le tokenizer ?\n",
        "Le tokenizer ajoute :\n",
        "- rigidit√©,\n",
        "- complexit√©,\n",
        "- comportements bizarres,\n",
        "- limitations pour certaines langues.\n",
        "\n",
        "Id√©alement, un futur LLM fonctionnerait directement :\n",
        "- au niveau des **caract√®res**,  \n",
        "ou mieux,\n",
        "- au niveau des **octets**,  \n",
        "sans √©tape de tokenisation.\n",
        "\n",
        "Certains mod√®les modernes explorent d√©j√† cette id√©e.\n"
      ],
      "metadata": {
        "id": "m6mm4A8E-oPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**_____Rappel ______**"
      ],
      "metadata": {
        "id": "QIfRnALl-shR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# R√©sum√© hyper simple du fonctionnement d‚Äôun Transformeur\n",
        "\n",
        "## 1. Tokenisation\n",
        "- Le texte est d√©coup√© en unit√©s appel√©es *tokens* (mots, sous-mots‚Ä¶).\n",
        "- Chaque token est converti en un entier (ID du vocabulaire).\n",
        "\n",
        "## 2. Embeddings + positions\n",
        "- Chaque ID est transform√© en un vecteur dense (*embedding*).\n",
        "- On ajoute un *positional embedding* pour indiquer la position du token.\n",
        "\n",
        "‚Üí R√©sultat : une s√©quence de vecteurs repr√©sentant **le sens + la position** des tokens.\n",
        "\n",
        "## 3. Self-Attention (comprendre le contexte)\n",
        "Chaque token ¬´ regarde ¬ª tous les autres pour comprendre leurs relations.\n",
        "\n",
        "√âtapes internes :\n",
        "1. Projection des vecteurs en Q (query), K (key), V (value).\n",
        "2. Calcul des similarit√©s entre Q et K.\n",
        "3. Application d‚Äôun *softmax* pour obtenir les poids d‚Äôattention.\n",
        "4. Combinaison des V selon ces poids.\n",
        "\n",
        "‚Üí Chaque token devient une repr√©sentation **contextuelle enrichie**.\n",
        "\n",
        "**Multi-Head Attention** : r√©p√©ter plusieurs fois l‚Äôattention en parall√®le puis combiner les r√©sultats.\n",
        "\n",
        "## 4. Feed-Forward (raffiner l‚Äôinformation)\n",
        "- Petit r√©seau : 2 couches lin√©aires + activation (ReLU, GELU‚Ä¶).\n",
        "- Ajout de :\n",
        "  - connexions r√©siduelles (*skip connections*) sortie = entr√©e_origine + transformation(entr√©e_origine)\n",
        "[Am√©liorer les gradients: Le gradient peut ‚Äúse propager‚Äù plus facilement en arri√®re (backpropagation), ce qui √©vite :\n",
        "le vanishing gradient (gradient trop faible),\n",
        "le exploding gradient (gradient trop fort).\n",
        "Aider le r√©seau √† apprendre plus facilement\n",
        "Emp√™cher que le signal disparaisse:En ajoutant un chemin direct, l‚Äôinformation circule mieux.]\n",
        "  - normalisation (*LayerNorm*).\n",
        "\n",
        "‚Üí Stabilise et am√©liore l‚Äôapprentissage.\n",
        "\n",
        "## 5. Empilement de couches\n",
        "- Les blocs ‚ÄúSelf-Attention + Feed-Forward‚Äù sont r√©p√©t√©s (12, 24, 96‚Ä¶).\n",
        "- On obtient une repr√©sentation finale riche du texte.\n",
        "\n",
        "## 6. Projection vers le vocabulaire (t√™te finale)\n",
        "\n",
        "### Mod√®les g√©n√©ratifs (GPT)\n",
        "- Le vecteur final de chaque token est projet√© vers le vocabulaire.\n",
        "- Le mod√®le pr√©dit le **token suivant**.\n",
        "\n",
        "### Mod√®les de classification (BERT)\n",
        "- On utilise le vecteur du token sp√©cial **[CLS]**.\n",
        "- On effectue une pr√©diction (sentiment, entit√©, etc.).\n",
        "\n",
        "## 7. D√©codage (si g√©n√©ration)\n",
        "- Conversion des logits en probabilit√©s.\n",
        "- Choix du token (greedy, sampling, beam search‚Ä¶).\n",
        "- Ajout du token √† la s√©quence.\n",
        "- R√©p√©tition du processus jusqu‚Äô√† la fin du texte.\n"
      ],
      "metadata": {
        "id": "KEqN5z6D84FU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "tefQorBzAqGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Let's build the GPT Tokenizer**"
      ],
      "metadata": {
        "id": "mDKdA7P6Arpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n"
      ],
      "metadata": {
        "id": "4Jw5AOWxApHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comme GPT-2, LLaMA-2 est **chunk-level**, pas character-level.\n",
        "\n",
        "##  GPT-2 paper : 50 257 tokens, contexte 1 024 tokens\n",
        "Dans *Language Models are Unsupervised Multitask Learners* (OpenAI, 2019)\n",
        "- **Vocabulaire : 50 257 tokens**\n",
        "- **Contexte maximal : 1 024 tokens**\n",
        "\n",
        "[tiktokenizer ](https://tiktokenizer.vercel.app/)\n",
        "## LLaMA-2 paper : m√™me logique, mais vocabulaire diff√©rent\n",
        "- vocabulaire ‚âà **32k tokens** (plus petit que GPT-2)\n",
        "- optimis√© pour plusieurs langues\n",
        "- bas√© sur SentencePiece (mont√© en BPE)\n",
        "\n"
      ],
      "metadata": {
        "id": "Yuz1PHfLBRr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "j'ai deja constater que gpt2 marche bien avec promt en anglais que d'autres langages √ßa revient aussi au probleme de son tokenizer\n",
        "\n",
        "- more long large tokens in eng\n",
        "- in coding handling space prob like in python"
      ],
      "metadata": {
        "id": "UummjApeFBc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we want to take strings and feed them into a language models\n",
        "#how tokenize strings into some integers in some fixed vocab\n",
        "#then we will use those integers to make a look up into a lookup table of vectors\n",
        "#and feed those vectors into the Transformer as an input\n",
        "#we want to support eng and no-english languages and specials caracteres like imogies"
      ],
      "metadata": {
        "id": "jV0UUBy9G-8Y"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "0RC4yrnr6ds9"
      },
      "outputs": [],
      "source": [
        "#str in python : Unicode : 149813 char\n",
        "#is not stable representation\n",
        "#UTF-8 ,UTF-16, UTF-32;\n",
        "#UTF-8 translate to Byte stream between 1 and 4 bytes paper. UTF-8 EVERYWHERE\n",
        "#ord() function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(\"ŸÖÿ±ÿ≠ÿ®ÿß bonjour üëãhello\".encode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5YCqJjKIi4A",
        "outputId": "00eb873c-948a-460f-d212-f51fe519c6ab"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[217,\n",
              " 133,\n",
              " 216,\n",
              " 177,\n",
              " 216,\n",
              " 173,\n",
              " 216,\n",
              " 168,\n",
              " 216,\n",
              " 167,\n",
              " 32,\n",
              " 98,\n",
              " 111,\n",
              " 110,\n",
              " 106,\n",
              " 111,\n",
              " 117,\n",
              " 114,\n",
              " 32,\n",
              " 240,\n",
              " 159,\n",
              " 145,\n",
              " 139,\n",
              " 104,\n",
              " 101,\n",
              " 108,\n",
              " 108,\n",
              " 111]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(\"ŸÖÿ±ÿ≠ÿ®ÿß bonjour üëãhello\".encode(\"utf-16\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aEDIfpUJF2K",
        "outputId": "29e9a3e0-7a1e-4ab6-8c21-85879b761039"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[255,\n",
              " 254,\n",
              " 69,\n",
              " 6,\n",
              " 49,\n",
              " 6,\n",
              " 45,\n",
              " 6,\n",
              " 40,\n",
              " 6,\n",
              " 39,\n",
              " 6,\n",
              " 32,\n",
              " 0,\n",
              " 98,\n",
              " 0,\n",
              " 111,\n",
              " 0,\n",
              " 110,\n",
              " 0,\n",
              " 106,\n",
              " 0,\n",
              " 111,\n",
              " 0,\n",
              " 117,\n",
              " 0,\n",
              " 114,\n",
              " 0,\n",
              " 32,\n",
              " 0,\n",
              " 61,\n",
              " 216,\n",
              " 75,\n",
              " 220,\n",
              " 104,\n",
              " 0,\n",
              " 101,\n",
              " 0,\n",
              " 108,\n",
              " 0,\n",
              " 108,\n",
              " 0,\n",
              " 111,\n",
              " 0]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.x wastefull"
      ],
      "metadata": {
        "id": "S_PXrSn5JJLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#stay with UTF-8\n",
        "# atttention these are by streams vocab 256 possible tokens it's a small vocab\n",
        "# all our text would be stratched out over very long sequences of bytes so the embedding table is going to be a tiny and the prediction at the top at the final layer is going to be very tiny but our sequences are very long and we have a pretty finite context length and the attention that we can support in a transformer for computational reasons and so we only have as much context length\n"
      ],
      "metadata": {
        "id": "yK0YDcqbJMPq"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Avec une tokenisation byte-level pure (UTF-8), chaque token est un octet.  \n",
        "‚Üí Le vocabulaire a **256 tokens** seulement (0‚Äì255).  \n",
        "- C‚Äôest un vocabulaire extr√™mement petit compar√© aux 30k‚Äì50k tokens de GPT ou LLaMA.\n",
        "Inconv√©nient principal :\n",
        "- **s√©quences de tokens tr√®s longues**\n",
        "-Chaque caract√®re UTF-8 devient 1 √† 4 bytes.  \n",
        "Chaque byte devient 1 token.\n",
        "-- le texte est **√©tir√© (‚Äústretched out‚Äù)** en √©norm√©ment de tokens  \n",
        "- un mot normal peut devenir 4‚Äì10 tokens  \n",
        "\n",
        "\n",
        "### Contexte limit√©\n",
        "- Les Transformers ont une **longueur de contexte finie**\n",
        "- S√©quences longues ‚Üí mod√®le ne peut pas voir tout le texte\n",
        "- Inefficace pour g√©rer de longs documents\n"
      ],
      "metadata": {
        "id": "I4s5x3T6LcZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #solution # Byte Pair Encoding (BPE) Algorithm\n"
      ],
      "metadata": {
        "id": "r8Uk5Gg1Lbxt"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Id√©e principale BPE\n",
        "- On commence avec tous les **caract√®res** comme tokens.\n",
        "- On regarde **les paires de tokens les plus fr√©quentes**.\n",
        "- On fusionne ces paires en un **nouveau token**.\n",
        "- On r√©p√®te jusqu‚Äô√† atteindre la taille de vocabulaire d√©sir√©e.\n",
        "\n",
        "‚Üí R√©sultat : un vocabulaire de **sous-mots optimis√©s** pour le texte.\n",
        "\n",
        "Texte : `\"low lower lowest\"`  \n",
        "BPE tokens finaux : `[\"low\", \"er\", \"low\", \"est\"]`"
      ],
      "metadata": {
        "id": "hpOQUvbHMlK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "voir article\n",
        "\n",
        "\n",
        "\n",
        "> MEGABYTE: Predinctiong Million-byte Sequences with Multiscale Transformers\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NWspn9bgMwpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
        "text = \"ÔºµÔΩéÔΩâÔΩÉÔΩèÔΩÑÔΩÖ! üÖ§üÖùüÖòüÖíüÖûüÖìüÖî‚ÄΩ üá∫‚Äåüá≥‚ÄåüáÆ‚Äåüá®‚Äåüá¥‚Äåüá©‚Äåüá™! üòÑ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to ‚Äúsupport Unicode‚Äù in our software (whatever that means‚Äîlike using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don‚Äôt blame programmers for still finding the whole thing mysterious, even 30 years after Unicode‚Äôs inception.\"\n",
        "tokens = text.encode(\"utf-8\") # raw bytes\n",
        "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
        "print('------------------------------------')\n",
        "print(text)\n",
        "print(\"length:\", len(text))\n",
        "print('-------------------------------------')\n",
        "print(tokens)\n",
        "print(\"length:\", len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SGs1EVCMqjt",
        "outputId": "90b54c31-6ab3-49ed-858e-d0b28267f644"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------\n",
            "ÔºµÔΩéÔΩâÔΩÉÔΩèÔΩÑÔΩÖ! üÖ§üÖùüÖòüÖíüÖûüÖìüÖî‚ÄΩ üá∫‚Äåüá≥‚ÄåüáÆ‚Äåüá®‚Äåüá¥‚Äåüá©‚Äåüá™! üòÑ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to ‚Äúsupport Unicode‚Äù in our software (whatever that means‚Äîlike using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don‚Äôt blame programmers for still finding the whole thing mysterious, even 30 years after Unicode‚Äôs inception.\n",
            "length: 533\n",
            "-------------------------------------\n",
            "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
            "length: 616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#in the more complex characters become multiple bytes\n",
        "#on veux appliquer principes de BPE\n",
        "#cherchant la paire that occur the most frequently\n",
        "def get_stats(ids):\n",
        "    counts = {}\n",
        "    for pair in zip(ids, ids[1:]):#cr√©e des paires cons√©cutives\n",
        "       # Pythonic way to iterate consecutive elements\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "stats = get_stats(tokens)\n",
        "print(stats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZjJZTu1PGaH",
        "outputId": "4bc990ce-ca8e-4dda-a445-1731321fa394"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(239, 188): 1, (188, 181): 1, (181, 239): 1, (239, 189): 6, (189, 142): 1, (142, 239): 1, (189, 137): 1, (137, 239): 1, (189, 131): 1, (131, 239): 1, (189, 143): 1, (143, 239): 1, (189, 132): 1, (132, 239): 1, (189, 133): 1, (133, 33): 1, (33, 32): 2, (32, 240): 3, (240, 159): 15, (159, 133): 7, (133, 164): 1, (164, 240): 1, (133, 157): 1, (157, 240): 1, (133, 152): 1, (152, 240): 1, (133, 146): 1, (146, 240): 1, (133, 158): 1, (158, 240): 1, (133, 147): 1, (147, 240): 1, (133, 148): 1, (148, 226): 1, (226, 128): 12, (128, 189): 1, (189, 32): 1, (159, 135): 7, (135, 186): 1, (186, 226): 1, (128, 140): 6, (140, 240): 6, (135, 179): 1, (179, 226): 1, (135, 174): 1, (174, 226): 1, (135, 168): 1, (168, 226): 1, (135, 180): 1, (180, 226): 1, (135, 169): 1, (169, 226): 1, (135, 170): 1, (170, 33): 1, (159, 152): 1, (152, 132): 1, (132, 32): 1, (32, 84): 1, (84, 104): 1, (104, 101): 6, (101, 32): 20, (32, 118): 1, (118, 101): 3, (101, 114): 6, (114, 121): 2, (121, 32): 2, (32, 110): 2, (110, 97): 1, (97, 109): 4, (109, 101): 6, (32, 115): 5, (115, 116): 5, (116, 114): 3, (114, 105): 4, (105, 107): 2, (107, 101): 2, (101, 115): 3, (115, 32): 10, (32, 102): 4, (102, 101): 1, (101, 97): 4, (97, 114): 7, (114, 32): 6, (32, 97): 10, (97, 110): 10, (110, 100): 6, (100, 32): 4, (97, 119): 1, (119, 101): 2, (32, 105): 6, (105, 110): 12, (110, 116): 4, (116, 111): 3, (111, 32): 3, (32, 116): 9, (116, 104): 8, (32, 104): 1, (114, 116): 3, (116, 115): 3, (32, 111): 4, (111, 102): 3, (102, 32): 2, (32, 112): 3, (112, 114): 2, (114, 111): 2, (111, 103): 2, (103, 114): 2, (114, 97): 2, (109, 109): 2, (114, 115): 3, (32, 119): 4, (119, 111): 1, (111, 114): 6, (114, 108): 1, (108, 100): 1, (100, 119): 1, (119, 105): 1, (105, 100): 2, (100, 101): 5, (101, 46): 1, (46, 32): 3, (32, 87): 1, (87, 101): 1, (97, 108): 2, (108, 108): 3, (108, 32): 3, (32, 107): 1, (107, 110): 1, (110, 111): 2, (111, 119): 1, (119, 32): 1, (111, 117): 4, (117, 103): 1, (103, 104): 2, (104, 116): 2, (116, 32): 6, (32, 226): 1, (128, 156): 1, (156, 115): 1, (115, 117): 2, (117, 112): 2, (112, 112): 2, (112, 111): 2, (32, 85): 4, (85, 110): 4, (110, 105): 4, (105, 99): 4, (99, 111): 4, (111, 100): 4, (101, 226): 2, (128, 157): 1, (157, 32): 1, (110, 32): 5, (117, 114): 1, (115, 111): 1, (102, 116): 2, (116, 119): 1, (119, 97): 1, (114, 101): 3, (32, 40): 1, (40, 119): 1, (119, 104): 2, (104, 97): 4, (97, 116): 3, (116, 101): 4, (101, 118): 2, (32, 109): 3, (110, 115): 2, (115, 226): 1, (128, 148): 1, (148, 108): 1, (108, 105): 2, (32, 117): 1, (117, 115): 5, (115, 105): 1, (110, 103): 6, (103, 32): 4, (119, 99): 1, (99, 104): 1, (114, 95): 1, (95, 116): 1, (102, 111): 2, (103, 115): 1, (115, 44): 4, (44, 32): 5, (32, 114): 2, (105, 103): 1, (116, 63): 1, (63, 41): 1, (41, 46): 1, (32, 66): 1, (66, 117): 1, (117, 116): 1, (32, 99): 2, (99, 97): 2, (32, 98): 3, (98, 101): 2, (97, 98): 1, (98, 115): 1, (114, 117): 1, (115, 101): 1, (101, 44): 1, (32, 100): 3, (100, 105): 2, (105, 118): 1, (118, 105): 1, (104, 111): 2, (115, 97): 1, (100, 45): 1, (45, 112): 1, (112, 97): 1, (97, 103): 1, (103, 101): 1, (32, 83): 1, (83, 116): 1, (116, 97): 2, (100, 97): 2, (114, 100): 1, (112, 108): 2, (108, 117): 1, (105, 116): 2, (100, 111): 2, (111, 122): 1, (122, 101): 1, (101, 110): 3, (108, 101): 3, (101, 109): 1, (110, 110): 1, (110, 101): 1, (101, 120): 1, (120, 101): 1, (101, 112): 2, (111, 116): 1, (109, 111): 1, (97, 32): 1, (32, 108): 1, (116, 116): 1, (116, 108): 1, (116, 105): 4, (105, 109): 1, (109, 105): 1, (103, 46): 1, (32, 73): 1, (73, 32): 1, (111, 110): 2, (110, 226): 1, (128, 153): 2, (153, 116): 1, (98, 108): 1, (108, 97): 1, (105, 108): 1, (102, 105): 1, (111, 108): 1, (104, 105): 1, (109, 121): 1, (121, 115): 1, (105, 111): 2, (32, 101): 1, (32, 51): 1, (51, 48): 1, (48, 32): 1, (32, 121): 1, (121, 101): 1, (97, 102): 1, (153, 115): 1, (110, 99): 1, (99, 101): 1, (112, 116): 1, (110, 46): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sorted(((v,k) for k,v in stats.items()), reverse=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RxYd4XHP8fT",
        "outputId": "8b8dd674-0bbe-4a5e-a5bf-d305439bb78a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(20, (101, 32)), (15, (240, 159)), (12, (226, 128)), (12, (105, 110)), (10, (115, 32)), (10, (97, 110)), (10, (32, 97)), (9, (32, 116)), (8, (116, 104)), (7, (159, 135)), (7, (159, 133)), (7, (97, 114)), (6, (239, 189)), (6, (140, 240)), (6, (128, 140)), (6, (116, 32)), (6, (114, 32)), (6, (111, 114)), (6, (110, 103)), (6, (110, 100)), (6, (109, 101)), (6, (104, 101)), (6, (101, 114)), (6, (32, 105)), (5, (117, 115)), (5, (115, 116)), (5, (110, 32)), (5, (100, 101)), (5, (44, 32)), (5, (32, 115)), (4, (116, 105)), (4, (116, 101)), (4, (115, 44)), (4, (114, 105)), (4, (111, 117)), (4, (111, 100)), (4, (110, 116)), (4, (110, 105)), (4, (105, 99)), (4, (104, 97)), (4, (103, 32)), (4, (101, 97)), (4, (100, 32)), (4, (99, 111)), (4, (97, 109)), (4, (85, 110)), (4, (32, 119)), (4, (32, 111)), (4, (32, 102)), (4, (32, 85)), (3, (118, 101)), (3, (116, 115)), (3, (116, 114)), (3, (116, 111)), (3, (114, 116)), (3, (114, 115)), (3, (114, 101)), (3, (111, 102)), (3, (111, 32)), (3, (108, 108)), (3, (108, 101)), (3, (108, 32)), (3, (101, 115)), (3, (101, 110)), (3, (97, 116)), (3, (46, 32)), (3, (32, 240)), (3, (32, 112)), (3, (32, 109)), (3, (32, 100)), (3, (32, 98)), (2, (128, 153)), (2, (121, 32)), (2, (119, 104)), (2, (119, 101)), (2, (117, 112)), (2, (116, 97)), (2, (115, 117)), (2, (114, 121)), (2, (114, 111)), (2, (114, 97)), (2, (112, 114)), (2, (112, 112)), (2, (112, 111)), (2, (112, 108)), (2, (111, 110)), (2, (111, 103)), (2, (110, 115)), (2, (110, 111)), (2, (109, 109)), (2, (108, 105)), (2, (107, 101)), (2, (105, 116)), (2, (105, 111)), (2, (105, 107)), (2, (105, 100)), (2, (104, 116)), (2, (104, 111)), (2, (103, 114)), (2, (103, 104)), (2, (102, 116)), (2, (102, 111)), (2, (102, 32)), (2, (101, 226)), (2, (101, 118)), (2, (101, 112)), (2, (100, 111)), (2, (100, 105)), (2, (100, 97)), (2, (99, 97)), (2, (98, 101)), (2, (97, 108)), (2, (33, 32)), (2, (32, 114)), (2, (32, 110)), (2, (32, 99)), (1, (239, 188)), (1, (189, 143)), (1, (189, 142)), (1, (189, 137)), (1, (189, 133)), (1, (189, 132)), (1, (189, 131)), (1, (189, 32)), (1, (188, 181)), (1, (186, 226)), (1, (181, 239)), (1, (180, 226)), (1, (179, 226)), (1, (174, 226)), (1, (170, 33)), (1, (169, 226)), (1, (168, 226)), (1, (164, 240)), (1, (159, 152)), (1, (158, 240)), (1, (157, 240)), (1, (157, 32)), (1, (156, 115)), (1, (153, 116)), (1, (153, 115)), (1, (152, 240)), (1, (152, 132)), (1, (148, 226)), (1, (148, 108)), (1, (147, 240)), (1, (146, 240)), (1, (143, 239)), (1, (142, 239)), (1, (137, 239)), (1, (135, 186)), (1, (135, 180)), (1, (135, 179)), (1, (135, 174)), (1, (135, 170)), (1, (135, 169)), (1, (135, 168)), (1, (133, 164)), (1, (133, 158)), (1, (133, 157)), (1, (133, 152)), (1, (133, 148)), (1, (133, 147)), (1, (133, 146)), (1, (133, 33)), (1, (132, 239)), (1, (132, 32)), (1, (131, 239)), (1, (128, 189)), (1, (128, 157)), (1, (128, 156)), (1, (128, 148)), (1, (122, 101)), (1, (121, 115)), (1, (121, 101)), (1, (120, 101)), (1, (119, 111)), (1, (119, 105)), (1, (119, 99)), (1, (119, 97)), (1, (119, 32)), (1, (118, 105)), (1, (117, 116)), (1, (117, 114)), (1, (117, 103)), (1, (116, 119)), (1, (116, 116)), (1, (116, 108)), (1, (116, 63)), (1, (115, 226)), (1, (115, 111)), (1, (115, 105)), (1, (115, 101)), (1, (115, 97)), (1, (114, 117)), (1, (114, 108)), (1, (114, 100)), (1, (114, 95)), (1, (112, 116)), (1, (112, 97)), (1, (111, 122)), (1, (111, 119)), (1, (111, 116)), (1, (111, 108)), (1, (110, 226)), (1, (110, 110)), (1, (110, 101)), (1, (110, 99)), (1, (110, 97)), (1, (110, 46)), (1, (109, 121)), (1, (109, 111)), (1, (109, 105)), (1, (108, 117)), (1, (108, 100)), (1, (108, 97)), (1, (107, 110)), (1, (105, 118)), (1, (105, 109)), (1, (105, 108)), (1, (105, 103)), (1, (104, 105)), (1, (103, 115)), (1, (103, 101)), (1, (103, 46)), (1, (102, 105)), (1, (102, 101)), (1, (101, 120)), (1, (101, 109)), (1, (101, 46)), (1, (101, 44)), (1, (100, 119)), (1, (100, 45)), (1, (99, 104)), (1, (99, 101)), (1, (98, 115)), (1, (98, 108)), (1, (97, 119)), (1, (97, 103)), (1, (97, 102)), (1, (97, 98)), (1, (97, 32)), (1, (95, 116)), (1, (87, 101)), (1, (84, 104)), (1, (83, 116)), (1, (73, 32)), (1, (66, 117)), (1, (63, 41)), (1, (51, 48)), (1, (48, 32)), (1, (45, 112)), (1, (41, 46)), (1, (40, 119)), (1, (32, 226)), (1, (32, 121)), (1, (32, 118)), (1, (32, 117)), (1, (32, 108)), (1, (32, 107)), (1, (32, 104)), (1, (32, 101)), (1, (32, 87)), (1, (32, 84)), (1, (32, 83)), (1, (32, 73)), (1, (32, 66)), (1, (32, 51)), (1, (32, 40))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "char()  , ord()"
      ],
      "metadata": {
        "id": "T9je2UxvQKtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# after menting a new token with the ID of 256 right because max actuelle est 255\n",
        "#(101, 32) = 256\n",
        "top_pair = max(stats, key=stats.get)\n",
        "top_pair"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d29S671KQNFQ",
        "outputId": "6a1cee60-367d-41a4-909b-3592a38836ea"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(101, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# on a creer une fonction merge par ce que a chaqye iteration on va merger\n",
        "def merge(ids, pair, idx):\n",
        "  # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
        "  newids = []\n",
        "  i = 0\n",
        "  while i < len(ids):\n",
        "    # if we are not at the very last position AND the pair matches, replace it\n",
        "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "      newids.append(idx)\n",
        "      i += 2\n",
        "    else:\n",
        "      newids.append(ids[i])\n",
        "      i += 1\n",
        "  return newids\n",
        "\n",
        "print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYS5rZurQ_C_",
        "outputId": "a5d33d10-0139-43dd-9544-7f668efcd47f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 6, 99, 9, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens2 = merge(tokens, top_pair, 256)\n",
        "print(tokens2)\n",
        "print(\"length:\", len(tokens2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NfH9awHRUDx",
        "outputId": "fb5c59a0-2f37-4f3c-dd1d-3f1582b6ea55"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 256, 118, 101, 114, 121, 32, 110, 97, 109, 256, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 256, 105, 110, 116, 111, 32, 116, 104, 256, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 256, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 256, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 256, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 256, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 256, 99, 97, 110, 32, 98, 256, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 256, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 256, 85, 110, 105, 99, 111, 100, 256, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 256, 109, 111, 114, 256, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 256, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 256, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 256, 119, 104, 111, 108, 256, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
            "length: 596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# de 616 tokens a 596\n",
        "stats = get_stats(tokens2)\n",
        "top_pair = max(stats, key=stats.get)\n",
        "top_pair"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sn1SuOLbRW-d",
        "outputId": "3a7993d8-a1ab-41d5-dbe2-4bce55ed1bf7"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(240, 159)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#la question how many times on repete le processus\n",
        "# heee bas voila un hyperparameter\n",
        "# the more steps we take the larger will be our vocab genre 256 257 ....\n",
        "# and the shorter will be our sequence # de 616 tokens a 596\n",
        "#so it's a hyperparameter we tune it and we find goog vocab sizes"
      ],
      "metadata": {
        "id": "pzz85LxCRmxG"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#travaillons sur text plus grand\n",
        "# making the training text longer to have more representative token statistics\n",
        "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
        "text = \"\"\"A Programmer‚Äôs Introduction to Unicode March 3, 2017 ¬∑ Coding ¬∑ 22 Comments  ÔºµÔΩéÔΩâÔΩÉÔΩèÔΩÑÔΩÖ! üÖ§üÖùüÖòüÖíüÖûüÖìüÖî‚ÄΩ üá∫\\u200cüá≥\\u200cüáÆ\\u200cüá®\\u200cüá¥\\u200cüá©\\u200cüá™! üòÑ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to ‚Äúsupport Unicode‚Äù in our software (whatever that means‚Äîlike using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don‚Äôt blame programmers for still finding the whole thing mysterious, even 30 years after Unicode‚Äôs inception.  A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, I‚Äôll give an introduction to it from a programmer‚Äôs point of view.  I‚Äôm going to focus on the character set and what‚Äôs involved in working with strings and files of Unicode text. However, in this article I‚Äôm not going to talk about fonts, text layout/shaping/rendering, or localization in detail‚Äîthose are separate issues, beyond my scope (and knowledge) here.  Diversity and Inherent Complexity The Unicode Codespace Codespace Allocation Scripts Usage Frequency Encodings UTF-8 UTF-16 Combining Marks Canonical Equivalence Normalization Forms Grapheme Clusters And More‚Ä¶ Diversity and Inherent Complexity As soon as you start to study Unicode, it becomes clear that it represents a large jump in complexity over character sets like ASCII that you may be more familiar with. It‚Äôs not just that Unicode contains a much larger number of characters, although that‚Äôs part of it. Unicode also has a great deal of internal structure, features, and special cases, making it much more than what one might expect a mere ‚Äúcharacter set‚Äù to be. We‚Äôll see some of that later in this article.  When confronting all this complexity, especially as an engineer, it‚Äôs hard not to find oneself asking, ‚ÄúWhy do we need all this? Is this really necessary? Couldn‚Äôt it be simplified?‚Äù  However, Unicode aims to faithfully represent the entire world‚Äôs writing systems. The Unicode Consortium‚Äôs stated goal is ‚Äúenabling people around the world to use computers in any language‚Äù. And as you might imagine, the diversity of written languages is immense! To date, Unicode supports 135 different scripts, covering some 1100 languages, and there‚Äôs still a long tail of over 100 unsupported scripts, both modern and historical, which people are still working to add.  Given this enormous diversity, it‚Äôs inevitable that representing it is a complicated project. Unicode embraces that diversity, and accepts the complexity inherent in its mission to include all human writing systems. It doesn‚Äôt make a lot of trade-offs in the name of simplification, and it makes exceptions to its own rules where necessary to further its mission.  Moreover, Unicode is committed not just to supporting texts in any single language, but also to letting multiple languages coexist within one text‚Äîwhich introduces even more complexity.  Most programming languages have libraries available to handle the gory low-level details of text manipulation, but as a programmer, you‚Äôll still need to know about certain Unicode features in order to know when and how to apply them. It may take some time to wrap your head around it all, but don‚Äôt be discouraged‚Äîthink about the billions of people for whom your software will be more accessible through supporting text in their language. Embrace the complexity!  The Unicode Codespace Let‚Äôs start with some general orientation. The basic elements of Unicode‚Äîits ‚Äúcharacters‚Äù, although that term isn‚Äôt quite right‚Äîare called code points. Code points are identified by number, customarily written in hexadecimal with the prefix ‚ÄúU+‚Äù, such as U+0041 ‚ÄúA‚Äù latin capital letter a or U+03B8 ‚ÄúŒ∏‚Äù greek small letter theta. Each code point also has a short name, and quite a few other properties, specified in the Unicode Character Database.  The set of all possible code points is called the codespace. The Unicode codespace consists of 1,114,112 code points. However, only 128,237 of them‚Äîabout 12% of the codespace‚Äîare actually assigned, to date. There‚Äôs plenty of room for growth! Unicode also reserves an additional 137,468 code points as ‚Äúprivate use‚Äù areas, which have no standardized meaning and are available for individual applications to define for their own purposes.  Codespace Allocation To get a feel for how the codespace is laid out, it‚Äôs helpful to visualize it. Below is a map of the entire codespace, with one pixel per code point. It‚Äôs arranged in tiles for visual coherence; each small square is 16√ó16 = 256 code points, and each large square is a ‚Äúplane‚Äù of 65,536 code points. There are 17 planes altogether.  Map of the Unicode codespace (click to zoom)  White represents unassigned space. Blue is assigned code points, green is private-use areas, and the small red area is surrogates (more about those later). As you can see, the assigned code points are distributed somewhat sparsely, but concentrated in the first three planes.  Plane 0 is also known as the ‚ÄúBasic Multilingual Plane‚Äù, or BMP. The BMP contains essentially all the characters needed for modern text in any script, including Latin, Cyrillic, Greek, Han (Chinese), Japanese, Korean, Arabic, Hebrew, Devanagari (Indian), and many more.  (In the past, the codespace was just the BMP and no more‚ÄîUnicode was originally conceived as a straightforward 16-bit encoding, with only 65,536 code points. It was expanded to its current size in 1996. However, the vast majority of code points in modern text belong to the BMP.)  Plane 1 contains historical scripts, such as Sumerian cuneiform and Egyptian hieroglyphs, as well as emoji and various other symbols. Plane 2 contains a large block of less-common and historical Han characters. The remaining planes are empty, except for a small number of rarely-used formatting characters in Plane 14; planes 15‚Äì16 are reserved entirely for private use.  Scripts Let‚Äôs zoom in on the first three planes, since that‚Äôs where the action is:  Map of scripts in Unicode planes 0‚Äì2 (click to zoom)  This map color-codes the 135 different scripts in Unicode. You can see how Han () and Korean () take up most of the range of the BMP (the left large square). By contrast, all of the European, Middle Eastern, and South Asian scripts fit into the first row of the BMP in this diagram.  Many areas of the codespace are adapted or copied from earlier encodings. For example, the first 128 code points of Unicode are just a copy of ASCII. This has clear benefits for compatibility‚Äîit‚Äôs easy to losslessly convert texts from smaller encodings into Unicode (and the other direction too, as long as no characters outside the smaller encoding are used).  Usage Frequency One more interesting way to visualize the codespace is to look at the distribution of usage‚Äîin other words, how often each code point is actually used in real-world texts. Below is a heat map of planes 0‚Äì2 based on a large sample of text from Wikipedia and Twitter (all languages). Frequency increases from black (never seen) through red and yellow to white.  Heat map of code point usage frequency in Unicode planes 0‚Äì2 (click to zoom)  You can see that the vast majority of this text sample lies in the BMP, with only scattered usage of code points from planes 1‚Äì2. The biggest exception is emoji, which show up here as the several bright squares in the bottom row of plane 1.  Encodings We‚Äôve seen that Unicode code points are abstractly identified by their index in the codespace, ranging from U+0000 to U+10FFFF. But how do code points get represented as bytes, in memory or in a file?  The most convenient, computer-friendliest (and programmer-friendliest) thing to do would be to just store the code point index as a 32-bit integer. This works, but it consumes 4 bytes per code point, which is sort of a lot. Using 32-bit ints for Unicode will cost you a bunch of extra storage, memory, and performance in bandwidth-bound scenarios, if you work with a lot of text.  Consequently, there are several more-compact encodings for Unicode. The 32-bit integer encoding is officially called UTF-32 (UTF = ‚ÄúUnicode Transformation Format‚Äù), but it‚Äôs rarely used for storage. At most, it comes up sometimes as a temporary internal representation, for examining or operating on the code points in a string.  Much more commonly, you‚Äôll see Unicode text encoded as either UTF-8 or UTF-16. These are both variable-length encodings, made up of 8-bit or 16-bit units, respectively. In these schemes, code points with smaller index values take up fewer bytes, which saves a lot of memory for typical texts. The trade-off is that processing UTF-8/16 texts is more programmatically involved, and likely slower.  UTF-8 In UTF-8, each code point is stored using 1 to 4 bytes, based on its index value.  UTF-8 uses a system of binary prefixes, in which the high bits of each byte mark whether it‚Äôs a single byte, the beginning of a multi-byte sequence, or a continuation byte; the remaining bits, concatenated, give the code point index. This table shows how it works:  UTF-8 (binary)\\tCode point (binary)\\tRange 0xxxxxxx\\txxxxxxx\\tU+0000‚ÄìU+007F 110xxxxx 10yyyyyy\\txxxxxyyyyyy\\tU+0080‚ÄìU+07FF 1110xxxx 10yyyyyy 10zzzzzz\\txxxxyyyyyyzzzzzz\\tU+0800‚ÄìU+FFFF 11110xxx 10yyyyyy 10zzzzzz 10wwwwww\\txxxyyyyyyzzzzzzwwwwww\\tU+10000‚ÄìU+10FFFF A handy property of UTF-8 is that code points below 128 (ASCII characters) are encoded as single bytes, and all non-ASCII code points are encoded using sequences of bytes 128‚Äì255. This has a couple of nice consequences. First, any strings or files out there that are already in ASCII can also be interpreted as UTF-8 without any conversion. Second, lots of widely-used string programming idioms‚Äîsuch as null termination, or delimiters (newlines, tabs, commas, slashes, etc.)‚Äîwill just work on UTF-8 strings. ASCII bytes never occur inside the encoding of non-ASCII code points, so searching byte-wise for a null terminator or a delimiter will do the right thing.  Thanks to this convenience, it‚Äôs relatively simple to extend legacy ASCII programs and APIs to handle UTF-8 strings. UTF-8 is very widely used in the Unix/Linux and Web worlds, and many programmers argue UTF-8 should be the default encoding everywhere.  However, UTF-8 isn‚Äôt a drop-in replacement for ASCII strings in all respects. For instance, code that iterates over the ‚Äúcharacters‚Äù in a string will need to decode UTF-8 and iterate over code points (or maybe grapheme clusters‚Äîmore about those later), not bytes. When you measure the ‚Äúlength‚Äù of a string, you‚Äôll need to think about whether you want the length in bytes, the length in code points, the width of the text when rendered, or something else.  UTF-16 The other encoding that you‚Äôre likely to encounter is UTF-16. It uses 16-bit words, with each code point stored as either 1 or 2 words.  Like UTF-8, we can express the UTF-16 encoding rules in the form of binary prefixes:  UTF-16 (binary)\\tCode point (binary)\\tRange xxxxxxxxxxxxxxxx\\txxxxxxxxxxxxxxxx\\tU+0000‚ÄìU+FFFF 110110xxxxxxxxxx 110111yyyyyyyyyy\\txxxxxxxxxxyyyyyyyyyy + 0x10000\\tU+10000‚ÄìU+10FFFF A more common way that people talk about UTF-16 encoding, though, is in terms of code points called ‚Äúsurrogates‚Äù. All the code points in the range U+D800‚ÄìU+DFFF‚Äîor in other words, the code points that match the binary prefixes 110110 and 110111 in the table above‚Äîare reserved specifically for UTF-16 encoding, and don‚Äôt represent any valid characters on their own. They‚Äôre only meant to occur in the 2-word encoding pattern above, which is called a ‚Äúsurrogate pair‚Äù. Surrogate code points are illegal in any other context! They‚Äôre not allowed in UTF-8 or UTF-32 at all.  Historically, UTF-16 is a descendant of the original, pre-1996 versions of Unicode, in which there were only 65,536 code points. The original intention was that there would be no different ‚Äúencodings‚Äù; Unicode was supposed to be a straightforward 16-bit character set. Later, the codespace was expanded to make room for a long tail of less-common (but still important) Han characters, which the Unicode designers didn‚Äôt originally plan for. Surrogates were then introduced, as‚Äîto put it bluntly‚Äîa kludge, allowing 16-bit encodings to access the new code points.  Today, Javascript uses UTF-16 as its standard string representation: if you ask for the length of a string, or iterate over it, etc., the result will be in UTF-16 words, with any code points outside the BMP expressed as surrogate pairs. UTF-16 is also used by the Microsoft Win32 APIs; though Win32 supports either 8-bit or 16-bit strings, the 8-bit version unaccountably still doesn‚Äôt support UTF-8‚Äîonly legacy code-page encodings, like ANSI. This leaves UTF-16 as the only way to get proper Unicode support in Windows. (Update: in Win10 version 1903, they finally added UTF-8 support to the 8-bit APIs! üòä)  By the way, UTF-16‚Äôs words can be stored either little-endian or big-endian. Unicode has no opinion on that issue, though it does encourage the convention of putting U+FEFF zero width no-break space at the top of a UTF-16 file as a byte-order mark, to disambiguate the endianness. (If the file doesn‚Äôt match the system‚Äôs endianness, the BOM will be decoded as U+FFFE, which isn‚Äôt a valid code point.)  Combining Marks In the story so far, we‚Äôve been focusing on code points. But in Unicode, a ‚Äúcharacter‚Äù can be more complicated than just an individual code point!  Unicode includes a system for dynamically composing characters, by combining multiple code points together. This is used in various ways to gain flexibility without causing a huge combinatorial explosion in the number of code points.  In European languages, for example, this shows up in the application of diacritics to letters. Unicode supports a wide range of diacritics, including acute and grave accents, umlauts, cedillas, and many more. All these diacritics can be applied to any letter of any alphabet‚Äîand in fact, multiple diacritics can be used on a single letter.  If Unicode tried to assign a distinct code point to every possible combination of letter and diacritics, things would rapidly get out of hand. Instead, the dynamic composition system enables you to construct the character you want, by starting with a base code point (the letter) and appending additional code points, called ‚Äúcombining marks‚Äù, to specify the diacritics. When a text renderer sees a sequence like this in a string, it automatically stacks the diacritics over or under the base letter to create a composed character.  For example, the accented character ‚Äú√Å‚Äù can be expressed as a string of two code points: U+0041 ‚ÄúA‚Äù latin capital letter a plus U+0301 ‚Äú‚óåÃÅ‚Äù combining acute accent. This string automatically gets rendered as a single character: ‚Äú√Å‚Äù.  Now, Unicode does also include many ‚Äúprecomposed‚Äù code points, each representing a letter with some combination of diacritics already applied, such as U+00C1 ‚Äú√Å‚Äù latin capital letter a with acute or U+1EC7 ‚Äú·ªá‚Äù latin small letter e with circumflex and dot below. I suspect these are mostly inherited from older encodings that were assimilated into Unicode, and kept around for compatibility. In practice, there are precomposed code points for most of the common letter-with-diacritic combinations in European-script languages, so they don‚Äôt use dynamic composition that much in typical text.  Still, the system of combining marks does allow for an arbitrary number of diacritics to be stacked on any base character. The reductio-ad-absurdum of this is Zalgo text, which works by ÕñÕüÕÖrÕûa·πãÃ´Ã†ÃñÕàÃódÕñÃªÃπ√≥mÃ™ÕôÕïÃóÃùƒºÕáÃ∞ÕìÃ≥Ã´√ΩÕìÃ•ÃüÕç ÃïsÃ´tÃ´Ã±ÕïÃóÃ∞ÃºÃòÕúaÃºÃ©ÕñÕáÃ†ÕàÃ£ÕùcÃôÕçkÃñÃ±ÃπÕçÕòiÃ¢nÃ®Ã∫ÃùÕáÕáÃüÕôƒ£Ã´ÃÆÕéÃªÃüÕÖ ÃïnÃºÃ∫ÕàÕûuÃÆÕômÃ∫Ã≠ÃüÃóÕûeÃûÕìÃ∞Ã§ÕìÃ´rÃµoÃñ·π∑s“âÃ™ÕçÃ≠Ã¨ÃùÃ§ ÃÆÕâÃùÃûÃóÃüÕ†dÃ¥ÃüÃúÃ±ÕïÕöiÕáÃ´ÃºÃØÃ≠ÃúÕ°·∏ÅÕôÃªÃºcÃ≤Ã≤ÃπrÃ®Ã†ÃπÃ£Ã∞Ã¶iÃ±tÃ§ÃªÃ§ÕçÕôÃòÃïiÃµÃúÃ≠Ã§Ã±ÕécÃµs ÕòoÃ±Ã≤ÕàÃôÕñÕáÃ≤Õ¢nÕò ÃúÕàeÃ¨Ã≤Ã†Ã©acÕïÃ∫Ã†ÕâhÃ∑Ã™ Ã∫Ã£ÕñÃ±·∏ªÃ´Ã¨ÃùÃπ·∏ôÃôÃ∫ÕôÃ≠ÕìÃ≤tÃûÃûÕáÃ≤ÕâÕçtÃ∑ÕîÃ™ÕâÃ≤ÃªÃ†ÕôeÃ¶ÃªÕàÕâÕárÕáÃ≠Ã≠Ã¨Õñ,ÃñÃÅ ÃúÕôÕìÃ£Ã≠sÃòÃòÕàoÃ±Ã∞Ã§Ã≤ÕÖ ÃõÃ¨ÃúÃôtÃºÃ¶ÕïÃ±ÃπÕïÃ•hÃ≥Ã≤ÕàÕùÕÖaÃ¶tÃªÃ≤ ÃªÃüÃ≠Ã¶ÃñtÃõÃ∞Ã©hÃ†ÕïÃ≥ÃùÃ´ÕïeÕàÃ§ÃòÕñÃûÕòy“âÃùÕô Ã∑ÕâÕîÃ∞Ã†oÃûÃ∞vÕàÕàÃ≥ÃòÕúerÃ∂fÃ∞ÕàÕî·∏ªÕïÃòÃ´Ã∫Ã≤oÃ≤Ã≠ÕôÕ†ÕÖwÃ±Ã≥Ã∫ ÕútÃ∏hÕáÃ≠ÕïÃ≥ÕçeÃñÃØÃüÃ† ÕçÃûÃúÕîÃ©Ã™ÕúƒºÕéÃ™Ã≤ÕöiÃùÃ≤ÃπÃôÃ©ÃπnÃ®Ã¶Ã©Ãñ·∏ôÃºÃ≤ÃºÕ¢ÕÖ Ã¨ÕùsÃºÕöÃòÃûÕùpÕôÃòÃªaÃôc“âÕâÃúÃ§ÕàÃØÃñiÃ•Õ°nÃ¶Ã†Ã±ÕügÃ∏ÃóÃªÃ¶Ã≠ÃÆÃüÕÖ Ã≥Ã™Ã†ÕñÃ≥ÃØÃïaÃ´ÕúnÕùdÕ° Ã£Ã¶ÃôÕÖcÃ™ÃórÃ¥ÕôÃÆÃ¶ÃπÃ≥eÕáÕöÃûÕîÃπÃ´ÕüaÃôÃ∫Ãô»õÕîÕéÃòÃπÕÖeÃ•Ã©Õç aÕñÃ™ÃúÃÆÕôÃπnÃ¢ÕâÃù ÕáÕâÕìÃ¶ÃºÃÅaÃ≥ÕñÃ™Ã§Ã±pÃñÕîÕîÃüÕáÕéÕ†pÃ±ÕçÃ∫ƒôÃ≤ÕéÕàÃ∞Ã≤Ã§Ã´aÃØÕúrÃ®ÃÆÃ´Ã£ÃòaÃ©ÃØÕñnÃπÃ¶Ã∞ÕéÃ£ÃûÃûcÃ®Ã¶Ã±ÕîÕéÕçÕñeÃ¨ÕìÕò Ã§Ã∞Ã©ÕôÃ§Ã¨ÕôoÃµÃºÃªÃ¨ÃªÕáÃÆÃ™fÃ¥ Ã°ÃôÃ≠ÕìÕñÃ™Ã§‚ÄúÃ∏ÕôÃ†ÃºcÃ≥ÃóÕúoÕèÃºÕôÕîÃÆrÃûÃ´Ã∫ÃûÃ•Ã¨ruÃ∫ÃªÃØÕâÃ≠ÃªÃØpÃ∞Ã•ÕìÃ£Ã´ÃôÃ§Õ¢tÃ≥ÕçÃ≥ÃñÕÖiÃ∂ÕàÃùÕôÃºÃôÃπoÃ°ÕînÃôÃ∫ÃπÃñÃ©ÕùÕÖ‚ÄùÃ®ÃóÕñÕöÃ©.ÃØÕì  A few other places where dynamic character composition shows up in Unicode:  Vowel-pointing notation in Arabic and Hebrew. In these languages, words are normally spelled with some of their vowels left out. They then have diacritic notation to indicate the vowels (used in dictionaries, language-teaching materials, children‚Äôs books, and such). These diacritics are expressed with combining marks.  A Hebrew example, with niqqud:\\t◊ê÷∂◊™ ◊ì÷∑◊ú÷∞◊™÷¥÷º◊ô ◊î÷µ◊ñ÷¥◊ô◊ñ ◊î÷µ◊†÷¥◊ô◊¢÷∑, ◊ß÷∂◊ò÷∂◊ë ◊ú÷¥◊©÷∞◊Å◊õ÷∑÷º◊™÷¥÷º◊ô ◊ô÷∏◊©◊Å◊ï÷π◊ì Normal writing (no niqqud):\\t◊ê◊™ ◊ì◊ú◊™◊ô ◊î◊ñ◊ô◊ñ ◊î◊†◊ô◊¢, ◊ß◊ò◊ë ◊ú◊©◊õ◊™◊ô ◊ô◊©◊ï◊ì Devanagari, the script used to write Hindi, Sanskrit, and many other South Asian languages, expresses certain vowels as combining marks attached to consonant letters. For example, ‚Äú‡§π‚Äù + ‚Äú\\u200b‡§ø‚Äù = ‚Äú‡§π‡§ø‚Äù (‚Äúh‚Äù + ‚Äúi‚Äù = ‚Äúhi‚Äù). Korean characters stand for syllables, but they are composed of letters called jamo that stand for the vowels and consonants in the syllable. While there are code points for precomposed Korean syllables, it‚Äôs also possible to dynamically compose them by concatenating their jamo. For example, ‚Äú·Ñí‚Äù + ‚Äú·Ö°‚Äù + ‚Äú·Ü´‚Äù = ‚ÄúÌïú‚Äù (‚Äúh‚Äù + ‚Äúa‚Äù + ‚Äún‚Äù = ‚Äúhan‚Äù). Canonical Equivalence In Unicode, precomposed characters exist alongside the dynamic composition system. A consequence of this is that there are multiple ways to express ‚Äúthe same‚Äù string‚Äîdifferent sequences of code points that result in the same user-perceived characters. For example, as we saw earlier, we can express the character ‚Äú√Å‚Äù either as the single code point U+00C1, or as the string of two code points U+0041 U+0301.  Another source of ambiguity is the ordering of multiple diacritics in a single character. Diacritic order matters visually when two diacritics apply to the same side of the base character, e.g. both above: ‚Äú«°‚Äù (dot, then macron) is different from ‚ÄúƒÅÃá‚Äù (macron, then dot). However, when diacritics apply to different sides of the character, e.g. one above and one below, then the order doesn‚Äôt affect rendering. Moreover, a character with multiple diacritics might have one of the diacritics precomposed and others expressed as combining marks.  For example, the Vietnamese letter ‚Äú·ªá‚Äù can be expressed in five different ways:  Fully precomposed: U+1EC7 ‚Äú·ªá‚Äù Partially precomposed: U+1EB9 ‚Äú·∫π‚Äù + U+0302 ‚Äú‚óåÃÇ‚Äù Partially precomposed: U+00EA ‚Äú√™‚Äù + U+0323 ‚Äú‚óåÃ£‚Äù Fully decomposed: U+0065 ‚Äúe‚Äù + U+0323 ‚Äú‚óåÃ£‚Äù + U+0302 ‚Äú‚óåÃÇ‚Äù Fully decomposed: U+0065 ‚Äúe‚Äù + U+0302 ‚Äú‚óåÃÇ‚Äù + U+0323 ‚Äú‚óåÃ£‚Äù Unicode refers to set of strings like this as ‚Äúcanonically equivalent‚Äù. Canonically equivalent strings are supposed to be treated as identical for purposes of searching, sorting, rendering, text selection, and so on. This has implications for how you implement operations on text. For example, if an app has a ‚Äúfind in file‚Äù operation and the user searches for ‚Äú·ªá‚Äù, it should, by default, find occurrences of any of the five versions of ‚Äú·ªá‚Äù above!  Normalization Forms To address the problem of ‚Äúhow to handle canonically equivalent strings‚Äù, Unicode defines several normalization forms: ways of converting strings into a canonical form so that they can be compared code-point-by-code-point (or byte-by-byte).  The ‚ÄúNFD‚Äù normalization form fully decomposes every character down to its component base and combining marks, taking apart any precomposed code points in the string. It also sorts the combining marks in each character according to their rendered position, so e.g. diacritics that go below the character come before the ones that go above the character. (It doesn‚Äôt reorder diacritics in the same rendered position, since their order matters visually, as previously mentioned.)  The ‚ÄúNFC‚Äù form, conversely, puts things back together into precomposed code points as much as possible. If an unusual combination of diacritics is called for, there may not be any precomposed code point for it, in which case NFC still precomposes what it can and leaves any remaining combining marks in place (again ordered by rendered position, as in NFD).  There are also forms called NFKD and NFKC. The ‚ÄúK‚Äù here refers to compatibility decompositions, which cover characters that are ‚Äúsimilar‚Äù in some sense but not visually identical. However, I‚Äôm not going to cover that here.  Grapheme Clusters As we‚Äôve seen, Unicode contains various cases where a thing that a user thinks of as a single ‚Äúcharacter‚Äù might actually be made up of multiple code points under the hood. Unicode formalizes this using the notion of a grapheme cluster: a string of one or more code points that constitute a single ‚Äúuser-perceived character‚Äù.  UAX #29 defines the rules for what, precisely, qualifies as a grapheme cluster. It‚Äôs approximately ‚Äúa base code point followed by any number of combining marks‚Äù, but the actual definition is a bit more complicated; it accounts for things like Korean jamo, and emoji ZWJ sequences.  The main thing grapheme clusters are used for is text editing: they‚Äôre often the most sensible unit for cursor placement and text selection boundaries. Using grapheme clusters for these purposes ensures that you can‚Äôt accidentally chop off some diacritics when you copy-and-paste text, that left/right arrow keys always move the cursor by one visible character, and so on.  Another place where grapheme clusters are useful is in enforcing a string length limit‚Äîsay, on a database field. While the true, underlying limit might be something like the byte length of the string in UTF-8, you wouldn‚Äôt want to enforce that by just truncating bytes. At a minimum, you‚Äôd want to ‚Äúround down‚Äù to the nearest code point boundary; but even better, round down to the nearest grapheme cluster boundary. Otherwise, you might be corrupting the last character by cutting off a diacritic, or interrupting a jamo sequence or ZWJ sequence.  And More‚Ä¶ There‚Äôs much more that could be said about Unicode from a programmer‚Äôs perspective! I haven‚Äôt gotten into such fun topics as case mapping, collation, compatibility decompositions and confusables, Unicode-aware regexes, or bidirectional text. Nor have I said anything yet about implementation issues‚Äîhow to efficiently store and look-up data about the sparsely-assigned code points, or how to optimize UTF-8 decoding, string comparison, or NFC normalization. Perhaps I‚Äôll return to some of those things in future posts.  Unicode is a fascinating and complex system. It has a many-to-one mapping between bytes and code points, and on top of that a many-to-one (or, under some circumstances, many-to-many) mapping between code points and ‚Äúcharacters‚Äù. It has oddball special cases in every corner. But no one ever claimed that representing all written languages was going to be easy, and it‚Äôs clear that we‚Äôre never going back to the bad old days of a patchwork of incompatible encodings.  Further reading:  The Unicode Standard UTF-8 Everywhere Manifesto Dark corners of Unicode by Eevee ICU (International Components for Unicode)‚ÄîC/C++/Java libraries implementing many Unicode algorithms and related things Python 3 Unicode Howto Google Noto Fonts‚Äîset of fonts intended to cover all assigned code points\"\"\"\n",
        "tokens = text.encode(\"utf-8\") # raw bytes\n",
        "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience"
      ],
      "metadata": {
        "id": "v8W23ac-RljM"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#t'inquiete j'ai juste copier the same functions\n",
        "def get_stats(ids):\n",
        "    counts = {}\n",
        "    for pair in zip(ids, ids[1:]):\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "def merge(ids, pair, idx):\n",
        "  newids = []\n",
        "  i = 0\n",
        "  while i < len(ids):\n",
        "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "      newids.append(idx)\n",
        "      i += 2\n",
        "    else:\n",
        "      newids.append(ids[i])\n",
        "      i += 1\n",
        "  return newids\n",
        "\n"
      ],
      "metadata": {
        "id": "5WZ_R-nHShep"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- essayons 276 = 20 merges\n",
        "vocab_size = 276 # the desired final vocabulary size\n",
        "num_merges = vocab_size - 256\n",
        "ids = list(tokens) # copy so we don't destroy the original list\n",
        "\n",
        "merges = {} # (int, int) -> int like #(101, 32) =  idx256\n",
        "for i in range(num_merges):\n",
        "  stats = get_stats(ids)\n",
        "  pair = max(stats, key=stats.get)\n",
        "  idx = 256 + i\n",
        "  print(f\"merging {pair} into a new token {idx}\")\n",
        "  ids = merge(ids, pair, idx)\n",
        "  merges[pair] = idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXgAI_GfSzoI",
        "outputId": "80bd995b-8340-43ac-eac6-4b84094a3871"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merging (101, 32) into a new token 256\n",
            "merging (105, 110) into a new token 257\n",
            "merging (115, 32) into a new token 258\n",
            "merging (116, 104) into a new token 259\n",
            "merging (101, 114) into a new token 260\n",
            "merging (99, 111) into a new token 261\n",
            "merging (116, 32) into a new token 262\n",
            "merging (226, 128) into a new token 263\n",
            "merging (44, 32) into a new token 264\n",
            "merging (97, 110) into a new token 265\n",
            "merging (111, 114) into a new token 266\n",
            "merging (100, 32) into a new token 267\n",
            "merging (97, 114) into a new token 268\n",
            "merging (101, 110) into a new token 269\n",
            "merging (257, 103) into a new token 270\n",
            "merging (261, 100) into a new token 271\n",
            "merging (121, 32) into a new token 272\n",
            "merging (46, 32) into a new token 273\n",
            "merging (97, 108) into a new token 274\n",
            "merging (259, 256) into a new token 275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"tokens length:\", len(tokens))\n",
        "print(\"ids length:\", len(ids))\n",
        "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkcooMr7Tc_M",
        "outputId": "f60a9357-2fd8-47a9-abb7-de73cdcad259"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens length: 24597\n",
            "ids length: 19438\n",
            "compression ratio: 1.27X\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "rappelons que tokenizer est totalement s√©par√© du LLM it's a separate object it has a different training set\n",
        "it is a pre processing stage that we would run a single time in the beginning\n",
        "and the tokenizer use BPE\n",
        "\n",
        "once it's trained : we have the vocab + merges = we can do encoding and decoding\n",
        "\n",
        "*so it's like a translation layer*"
      ],
      "metadata": {
        "id": "eo-4nJW8Tn9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#text -> Tokenizer -> tokens\n",
        "# tokens -> Tokenizer -> text\n",
        "\n",
        "# we care about multi many different languages and is it a code or not\n",
        "# the training set determine how many merges of it there will be"
      ],
      "metadata": {
        "id": "r8E-v2f7Uit4"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **encoding + decoding**\n",
        "text -> Tokenizer -> tokens\n",
        "\n",
        "tokens -> Tokenizer -> text\n",
        "\n"
      ],
      "metadata": {
        "id": "nb-S4InBWKBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**decoding**"
      ],
      "metadata": {
        "id": "GcXmQoGIWXsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "#vocabulaire de bytes pour tous les octets possibles (0‚Äì255).\n",
        "for (p0, p1), idx in merges.items():\n",
        "  #merges = dictionnaire qui contient les fusions BPE : (20 items) qu'on a merger {(101,32):256,....}\n",
        "  #cl√© = paire de tokens (p0, p1) and idx val\n",
        "    vocab[idx] = vocab[p0] + vocab[p1]\n",
        "  #On construit le nouveau token fusionn√© en concat√©nant les bytes existants\n",
        "\n",
        "#vocab contient les tokens initiaux + tous les sous-mots fusionn√©s.\n",
        "def decode(ids):\n",
        "  # given ids (list of integers), return Python string\n",
        "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
        "  text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
        "  return text\n",
        "\n",
        "print(decode([128])) #ÔøΩ\n",
        "#decode [97] a but , [128] error can't decode 0x80 in position0\n",
        "#sol : use  , errors=\"replace\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVOjOjzTWXNk",
        "outputId": "94cd1e2c-92d0-4fca-f820-f8e6d06fd18e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÔøΩ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#vocab contient les tokens initiaux + tous les sous-mots fusionn√©s.\n",
        "print(decode([259]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJSIk2jqbBIT",
        "outputId": "277806ee-a42b-46cc-dbfd-3a42b20347b7"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**encoding**"
      ],
      "metadata": {
        "id": "rnyVsJuUbXcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merges"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-ozpBSXcCrq",
        "outputId": "1af34e28-8a4c-45df-f7a8-d1cf079458b0"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(101, 32): 256,\n",
              " (105, 110): 257,\n",
              " (115, 32): 258,\n",
              " (116, 104): 259,\n",
              " (101, 114): 260,\n",
              " (99, 111): 261,\n",
              " (116, 32): 262,\n",
              " (226, 128): 263,\n",
              " (44, 32): 264,\n",
              " (97, 110): 265,\n",
              " (111, 114): 266,\n",
              " (100, 32): 267,\n",
              " (97, 114): 268,\n",
              " (101, 110): 269,\n",
              " (257, 103): 270,\n",
              " (261, 100): 271,\n",
              " (121, 32): 272,\n",
              " (46, 32): 273,\n",
              " (97, 108): 274,\n",
              " (259, 256): 275}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#comment on va encoder on peut utiliser au debut encodage utf 8 mais\n",
        "#on doit utiliser aussi les merges mais comment y a un ordre a respecter genre\n",
        "# comme de aaabdaaabac -> ZabdZabac -> ZYdZYac -> XdXac\n",
        "# chercher (101, 32)  ..... avant  (259, 256)\n",
        "#respecter ordre est simple voir the key 257<259\n"
      ],
      "metadata": {
        "id": "_HJEAIVZcE4r"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text):\n",
        "  # given a string, return list of integers (the tokens)\n",
        "  tokens = list(text.encode(\"utf-8\"))\n",
        "  while len(tokens) >= 2: #eviter erreur encode(\"a\")\n",
        "    stats = get_stats(tokens)\n",
        "    #respecter ordre est simple voir the key 257<259\n",
        "    pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
        "    if pair not in merges:\n",
        "      break # nothing else can be merged\n",
        "    idx = merges[pair]\n",
        "    tokens = merge(tokens, pair, idx)\n",
        "  return tokens\n",
        "\n",
        "print(encode(\"hi it's a\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwgmubicbtbI",
        "outputId": "cf8ca6e7-ff3a-4872-a44b-4b55cc30b967"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[104, 105, 32, 105, 116, 39, 258, 97]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(encode(\"hi it's a\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LIYMBEqedX2",
        "outputId": "e042d5ad-f6e1-44fa-ef3e-d9ee42aa4417"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi it's a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valtext =  \"On a $50,000 mortgage of 30 years at 8 percent, the monthly payment would be $366.88.\"\n",
        "valtext2 = decode(encode(valtext))\n",
        "print(valtext2 == valtext)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9CnNsb9enD1",
        "outputId": "df4b240a-33fa-4c2f-b4dc-8db20e7ff2c1"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "2zd5Op19fDQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forced splits using regex patterns (GPT series)\n",
        "\n",
        "> Language Models are Unsupervised Multitask Learners\n",
        "\n",
        "dog. dog dog! it's like semantic+ pontuation = suboptimal\n",
        "some type of caracters should never be merged together\n",
        "\n",
        "\n",
        "\n",
        "> openai/gpt-2\n",
        "\n",
        "```\n",
        "# Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
        "\n",
        "self.pat = re.compile(r\"''|``|\\'|\\\"|m|ll|d|re|s|ve|t|em|m'\\w+|['\\p{L}]+|‚Ä¶|[A-Za-z]+|(\\w+)|...?\")\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qfrV2_XEfG_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "#lowercase\n",
        "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "#d√©couper le texte en tokens avant d‚Äôappliquer BPE\n",
        "print(re.findall(gpt2pat, \"Hello've world123 how's are you!!!?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3v0vvIdfDpn",
        "outputId": "792919bd-78cc-4bd9-cf43-eeb36dc5b68d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', \"'ve\", ' world', '123', ' how', \"'s\", ' are', ' you', '!!!?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "L‚Äôexpression d√©coupe le texte en tokens adapt√©s √† GPT-2 :\n",
        "\n",
        "Contractions anglaises ‚Üí 's, 're\n",
        "\n",
        "Mots ‚Üí lettres avec espace optionnel\n",
        "\n",
        "Nombres ‚Üí chiffres avec espace optionnel\n",
        "\n",
        "Symboles/punctuations ‚Üí avec espace optionnel\n",
        "\n",
        "Espaces isol√©s ‚Üí pr√©serv√©s"
      ],
      "metadata": {
        "id": "OG5aJH6SiSYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"\"\"\n",
        "for i in range(1, 101):\n",
        "    if i % 3 == 0 and i % 5 == 0:\n",
        "        print(\"FizzBuzz\")\n",
        "    elif i % 3 == 0:\n",
        "        print(\"Fizz\")\n",
        "    elif i % 5 == 0:\n",
        "        print(\"Buzz\")\n",
        "    else:\n",
        "        print(i)\n",
        "\"\"\"\n",
        "print(re.findall(gpt2pat, example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0oALO7Ig-p0",
        "outputId": "3e39346d-cc44-43c2-f468-9143357062fb"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', 'for', ' i', ' in', ' range', '(', '1', ',', ' 101', '):', '\\n   ', ' if', ' i', ' %', ' 3', ' ==', ' 0', ' and', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'FizzBuzz', '\")', '\\n   ', ' elif', ' i', ' %', ' 3', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Fizz', '\")', '\\n   ', ' elif', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Buzz', '\")', '\\n   ', ' else', ':', '\\n       ', ' print', '(', 'i', ')', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#white space is included at the beginning, but if there are multiple spaces, it preserves them."
      ],
      "metadata": {
        "id": "TWkD75V9jNxt"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken # added for colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "numzLZqqhOZ7",
        "outputId": "12499ffa-e4eb-4e89-bcdc-ee9e33ef4e33"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in gpt4 they change the regular expression that they use to chunk up the text\n",
        "# not more than 3 numbers merging\n",
        "# upper and lower\n",
        "# on va appliquer leurs algo sur notre vocab et merges\n",
        "# ils ont byte_encoder byte_decoder layers entre Tokenizer et raw_text"
      ],
      "metadata": {
        "id": "S6Pxx1oZkC7N"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "# GPT-2 (does not merge spaces)\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "print(enc.encode(\"    hello world!!!\"))\n",
        "\n",
        "# GPT-4 (merges spaces)\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "print(enc.encode(\"    hello world!!!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX6hmcYhne9A",
        "outputId": "de891e61-fa01-4d9b-924d-d76164a317e0"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[220, 220, 220, 23748, 995, 10185]\n",
            "[262, 24748, 1917, 12340]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
        "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmzW9Yvinbp4",
        "outputId": "9fb87f87-6639-43fa-db20-1d29801af23c"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-08 20:49:43--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 20.60.241.33\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.60.241.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 456318 (446K) [application/octet-stream]\n",
            "Saving to: ‚Äòvocab.bpe.1‚Äô\n",
            "\n",
            "vocab.bpe.1         100%[===================>] 445.62K  1.63MB/s    in 0.3s    \n",
            "\n",
            "2025-12-08 20:49:44 (1.63 MB/s) - ‚Äòvocab.bpe.1‚Äô saved [456318/456318]\n",
            "\n",
            "--2025-12-08 20:49:44--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 20.60.241.33\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.60.241.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1042301 (1018K) [application/json]\n",
            "Saving to: ‚Äòencoder.json.1‚Äô\n",
            "\n",
            "encoder.json.1      100%[===================>]   1018K  2.27MB/s    in 0.4s    \n",
            "\n",
            "2025-12-08 20:49:44 (2.27 MB/s) - ‚Äòencoder.json.1‚Äô saved [1042301/1042301]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "\n",
        "with open('encoder.json', 'r') as f:\n",
        "    encoder = json.load(f) # <--- ~equivalent to our \"vocab\"\n",
        "\n",
        "with open('vocab.bpe', 'r', encoding=\"utf-8\") as f:\n",
        "    bpe_data = f.read()\n",
        "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
        "# ^---- ~equivalent to our \"merges\"\n"
      ],
      "metadata": {
        "id": "YxmktXg9kSXg"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoder.json contains a mapping of tokens ‚Üí integer IDs\n",
        "#n GPT/BPE terminology, this is the merges dictionary.\n",
        "\n",
        "##Each pair will be merged iteratively during tokenization.\n",
        "\n",
        "### bpr cherche les bigramm donc les paires et merge selon la freq"
      ],
      "metadata": {
        "id": "BaSvZ_Rmn2dK"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**special tokens**"
      ],
      "metadata": {
        "id": "ojkU54jckByQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(encoder) )\n",
        "# 256 raw byte tokens.\n",
        "#50,000 merges.\n",
        "#+1 special token\n",
        "encoder['<|endoftext|>']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nqanp0GIobiD",
        "outputId": "12a6f303-0201-4c9a-d392-02a07ee25eb2"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50257\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50256"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSqucQPLohgX",
        "outputId": "a270fbb0-55b2-44ad-fbe8-7cea9d071041"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'!': 0,\n",
              " '\"': 1,\n",
              " '#': 2,\n",
              " '$': 3,\n",
              " '%': 4,\n",
              " '&': 5,\n",
              " \"'\": 6,\n",
              " '(': 7,\n",
              " ')': 8,\n",
              " '*': 9,\n",
              " '+': 10,\n",
              " ',': 11,\n",
              " '-': 12,\n",
              " '.': 13,\n",
              " '/': 14,\n",
              " '0': 15,\n",
              " '1': 16,\n",
              " '2': 17,\n",
              " '3': 18,\n",
              " '4': 19,\n",
              " '5': 20,\n",
              " '6': 21,\n",
              " '7': 22,\n",
              " '8': 23,\n",
              " '9': 24,\n",
              " ':': 25,\n",
              " ';': 26,\n",
              " '<': 27,\n",
              " '=': 28,\n",
              " '>': 29,\n",
              " '?': 30,\n",
              " '@': 31,\n",
              " 'A': 32,\n",
              " 'B': 33,\n",
              " 'C': 34,\n",
              " 'D': 35,\n",
              " 'E': 36,\n",
              " 'F': 37,\n",
              " 'G': 38,\n",
              " 'H': 39,\n",
              " 'I': 40,\n",
              " 'J': 41,\n",
              " 'K': 42,\n",
              " 'L': 43,\n",
              " 'M': 44,\n",
              " 'N': 45,\n",
              " 'O': 46,\n",
              " 'P': 47,\n",
              " 'Q': 48,\n",
              " 'R': 49,\n",
              " 'S': 50,\n",
              " 'T': 51,\n",
              " 'U': 52,\n",
              " 'V': 53,\n",
              " 'W': 54,\n",
              " 'X': 55,\n",
              " 'Y': 56,\n",
              " 'Z': 57,\n",
              " '[': 58,\n",
              " '\\\\': 59,\n",
              " ']': 60,\n",
              " '^': 61,\n",
              " '_': 62,\n",
              " '`': 63,\n",
              " 'a': 64,\n",
              " 'b': 65,\n",
              " 'c': 66,\n",
              " 'd': 67,\n",
              " 'e': 68,\n",
              " 'f': 69,\n",
              " 'g': 70,\n",
              " 'h': 71,\n",
              " 'i': 72,\n",
              " 'j': 73,\n",
              " 'k': 74,\n",
              " 'l': 75,\n",
              " 'm': 76,\n",
              " 'n': 77,\n",
              " 'o': 78,\n",
              " 'p': 79,\n",
              " 'q': 80,\n",
              " 'r': 81,\n",
              " 's': 82,\n",
              " 't': 83,\n",
              " 'u': 84,\n",
              " 'v': 85,\n",
              " 'w': 86,\n",
              " 'x': 87,\n",
              " 'y': 88,\n",
              " 'z': 89,\n",
              " '{': 90,\n",
              " '|': 91,\n",
              " '}': 92,\n",
              " '~': 93,\n",
              " '¬°': 94,\n",
              " '¬¢': 95,\n",
              " '¬£': 96,\n",
              " '¬§': 97,\n",
              " '¬•': 98,\n",
              " '¬¶': 99,\n",
              " '¬ß': 100,\n",
              " '¬®': 101,\n",
              " '¬©': 102,\n",
              " '¬™': 103,\n",
              " '¬´': 104,\n",
              " '¬¨': 105,\n",
              " '¬Æ': 106,\n",
              " '¬Ø': 107,\n",
              " '¬∞': 108,\n",
              " '¬±': 109,\n",
              " '¬≤': 110,\n",
              " '¬≥': 111,\n",
              " '¬¥': 112,\n",
              " '¬µ': 113,\n",
              " '¬∂': 114,\n",
              " '¬∑': 115,\n",
              " '¬∏': 116,\n",
              " '¬π': 117,\n",
              " '¬∫': 118,\n",
              " '¬ª': 119,\n",
              " '¬º': 120,\n",
              " '¬Ω': 121,\n",
              " '¬æ': 122,\n",
              " '¬ø': 123,\n",
              " '√Ä': 124,\n",
              " '√Å': 125,\n",
              " '√Ç': 126,\n",
              " '√É': 127,\n",
              " '√Ñ': 128,\n",
              " '√Ö': 129,\n",
              " '√Ü': 130,\n",
              " '√á': 131,\n",
              " '√à': 132,\n",
              " '√â': 133,\n",
              " '√ä': 134,\n",
              " '√ã': 135,\n",
              " '√å': 136,\n",
              " '√ç': 137,\n",
              " '√é': 138,\n",
              " '√è': 139,\n",
              " '√ê': 140,\n",
              " '√ë': 141,\n",
              " '√í': 142,\n",
              " '√ì': 143,\n",
              " '√î': 144,\n",
              " '√ï': 145,\n",
              " '√ñ': 146,\n",
              " '√ó': 147,\n",
              " '√ò': 148,\n",
              " '√ô': 149,\n",
              " '√ö': 150,\n",
              " '√õ': 151,\n",
              " '√ú': 152,\n",
              " '√ù': 153,\n",
              " '√û': 154,\n",
              " '√ü': 155,\n",
              " '√†': 156,\n",
              " '√°': 157,\n",
              " '√¢': 158,\n",
              " '√£': 159,\n",
              " '√§': 160,\n",
              " '√•': 161,\n",
              " '√¶': 162,\n",
              " '√ß': 163,\n",
              " '√®': 164,\n",
              " '√©': 165,\n",
              " '√™': 166,\n",
              " '√´': 167,\n",
              " '√¨': 168,\n",
              " '√≠': 169,\n",
              " '√Æ': 170,\n",
              " '√Ø': 171,\n",
              " '√∞': 172,\n",
              " '√±': 173,\n",
              " '√≤': 174,\n",
              " '√≥': 175,\n",
              " '√¥': 176,\n",
              " '√µ': 177,\n",
              " '√∂': 178,\n",
              " '√∑': 179,\n",
              " '√∏': 180,\n",
              " '√π': 181,\n",
              " '√∫': 182,\n",
              " '√ª': 183,\n",
              " '√º': 184,\n",
              " '√Ω': 185,\n",
              " '√æ': 186,\n",
              " '√ø': 187,\n",
              " 'ƒÄ': 188,\n",
              " 'ƒÅ': 189,\n",
              " 'ƒÇ': 190,\n",
              " 'ƒÉ': 191,\n",
              " 'ƒÑ': 192,\n",
              " 'ƒÖ': 193,\n",
              " 'ƒÜ': 194,\n",
              " 'ƒá': 195,\n",
              " 'ƒà': 196,\n",
              " 'ƒâ': 197,\n",
              " 'ƒä': 198,\n",
              " 'ƒã': 199,\n",
              " 'ƒå': 200,\n",
              " 'ƒç': 201,\n",
              " 'ƒé': 202,\n",
              " 'ƒè': 203,\n",
              " 'ƒê': 204,\n",
              " 'ƒë': 205,\n",
              " 'ƒí': 206,\n",
              " 'ƒì': 207,\n",
              " 'ƒî': 208,\n",
              " 'ƒï': 209,\n",
              " 'ƒñ': 210,\n",
              " 'ƒó': 211,\n",
              " 'ƒò': 212,\n",
              " 'ƒô': 213,\n",
              " 'ƒö': 214,\n",
              " 'ƒõ': 215,\n",
              " 'ƒú': 216,\n",
              " 'ƒù': 217,\n",
              " 'ƒû': 218,\n",
              " 'ƒü': 219,\n",
              " 'ƒ†': 220,\n",
              " 'ƒ°': 221,\n",
              " 'ƒ¢': 222,\n",
              " 'ƒ£': 223,\n",
              " 'ƒ§': 224,\n",
              " 'ƒ•': 225,\n",
              " 'ƒ¶': 226,\n",
              " 'ƒß': 227,\n",
              " 'ƒ®': 228,\n",
              " 'ƒ©': 229,\n",
              " 'ƒ™': 230,\n",
              " 'ƒ´': 231,\n",
              " 'ƒ¨': 232,\n",
              " 'ƒ≠': 233,\n",
              " 'ƒÆ': 234,\n",
              " 'ƒØ': 235,\n",
              " 'ƒ∞': 236,\n",
              " 'ƒ±': 237,\n",
              " 'ƒ≤': 238,\n",
              " 'ƒ≥': 239,\n",
              " 'ƒ¥': 240,\n",
              " 'ƒµ': 241,\n",
              " 'ƒ∂': 242,\n",
              " 'ƒ∑': 243,\n",
              " 'ƒ∏': 244,\n",
              " 'ƒπ': 245,\n",
              " 'ƒ∫': 246,\n",
              " 'ƒª': 247,\n",
              " 'ƒº': 248,\n",
              " 'ƒΩ': 249,\n",
              " 'ƒæ': 250,\n",
              " 'ƒø': 251,\n",
              " '≈Ä': 252,\n",
              " '≈Å': 253,\n",
              " '≈Ç': 254,\n",
              " '≈É': 255,\n",
              " 'ƒ†t': 256,\n",
              " 'ƒ†a': 257,\n",
              " 'he': 258,\n",
              " 'in': 259,\n",
              " 're': 260,\n",
              " 'on': 261,\n",
              " 'ƒ†the': 262,\n",
              " 'er': 263,\n",
              " 'ƒ†s': 264,\n",
              " 'at': 265,\n",
              " 'ƒ†w': 266,\n",
              " 'ƒ†o': 267,\n",
              " 'en': 268,\n",
              " 'ƒ†c': 269,\n",
              " 'it': 270,\n",
              " 'is': 271,\n",
              " 'an': 272,\n",
              " 'or': 273,\n",
              " 'es': 274,\n",
              " 'ƒ†b': 275,\n",
              " 'ed': 276,\n",
              " 'ƒ†f': 277,\n",
              " 'ing': 278,\n",
              " 'ƒ†p': 279,\n",
              " 'ou': 280,\n",
              " 'ƒ†an': 281,\n",
              " 'al': 282,\n",
              " 'ar': 283,\n",
              " 'ƒ†to': 284,\n",
              " 'ƒ†m': 285,\n",
              " 'ƒ†of': 286,\n",
              " 'ƒ†in': 287,\n",
              " 'ƒ†d': 288,\n",
              " 'ƒ†h': 289,\n",
              " 'ƒ†and': 290,\n",
              " 'ic': 291,\n",
              " 'as': 292,\n",
              " 'le': 293,\n",
              " 'ƒ†th': 294,\n",
              " 'ion': 295,\n",
              " 'om': 296,\n",
              " 'll': 297,\n",
              " 'ent': 298,\n",
              " 'ƒ†n': 299,\n",
              " 'ƒ†l': 300,\n",
              " 'st': 301,\n",
              " 'ƒ†re': 302,\n",
              " 've': 303,\n",
              " 'ƒ†e': 304,\n",
              " 'ro': 305,\n",
              " 'ly': 306,\n",
              " 'ƒ†be': 307,\n",
              " 'ƒ†g': 308,\n",
              " 'ƒ†T': 309,\n",
              " 'ct': 310,\n",
              " 'ƒ†S': 311,\n",
              " 'id': 312,\n",
              " 'ot': 313,\n",
              " 'ƒ†I': 314,\n",
              " 'ut': 315,\n",
              " 'et': 316,\n",
              " 'ƒ†A': 317,\n",
              " 'ƒ†is': 318,\n",
              " 'ƒ†on': 319,\n",
              " 'im': 320,\n",
              " 'am': 321,\n",
              " 'ow': 322,\n",
              " 'ay': 323,\n",
              " 'ad': 324,\n",
              " 'se': 325,\n",
              " 'ƒ†that': 326,\n",
              " 'ƒ†C': 327,\n",
              " 'ig': 328,\n",
              " 'ƒ†for': 329,\n",
              " 'ac': 330,\n",
              " 'ƒ†y': 331,\n",
              " 'ver': 332,\n",
              " 'ur': 333,\n",
              " 'ƒ†u': 334,\n",
              " 'ld': 335,\n",
              " 'ƒ†st': 336,\n",
              " 'ƒ†M': 337,\n",
              " \"'s\": 338,\n",
              " 'ƒ†he': 339,\n",
              " 'ƒ†it': 340,\n",
              " 'ation': 341,\n",
              " 'ith': 342,\n",
              " 'ir': 343,\n",
              " 'ce': 344,\n",
              " 'ƒ†you': 345,\n",
              " 'il': 346,\n",
              " 'ƒ†B': 347,\n",
              " 'ƒ†wh': 348,\n",
              " 'ol': 349,\n",
              " 'ƒ†P': 350,\n",
              " 'ƒ†with': 351,\n",
              " 'ƒ†1': 352,\n",
              " 'ter': 353,\n",
              " 'ch': 354,\n",
              " 'ƒ†as': 355,\n",
              " 'ƒ†we': 356,\n",
              " 'ƒ†(': 357,\n",
              " 'nd': 358,\n",
              " 'ill': 359,\n",
              " 'ƒ†D': 360,\n",
              " 'if': 361,\n",
              " 'ƒ†2': 362,\n",
              " 'ag': 363,\n",
              " 'ers': 364,\n",
              " 'ke': 365,\n",
              " 'ƒ†\"': 366,\n",
              " 'ƒ†H': 367,\n",
              " 'em': 368,\n",
              " 'ƒ†con': 369,\n",
              " 'ƒ†W': 370,\n",
              " 'ƒ†R': 371,\n",
              " 'her': 372,\n",
              " 'ƒ†was': 373,\n",
              " 'ƒ†r': 374,\n",
              " 'od': 375,\n",
              " 'ƒ†F': 376,\n",
              " 'ul': 377,\n",
              " 'ate': 378,\n",
              " 'ƒ†at': 379,\n",
              " 'ri': 380,\n",
              " 'pp': 381,\n",
              " 'ore': 382,\n",
              " 'ƒ†The': 383,\n",
              " 'ƒ†se': 384,\n",
              " 'us': 385,\n",
              " 'ƒ†pro': 386,\n",
              " 'ƒ†ha': 387,\n",
              " 'um': 388,\n",
              " 'ƒ†are': 389,\n",
              " 'ƒ†de': 390,\n",
              " 'ain': 391,\n",
              " 'and': 392,\n",
              " 'ƒ†or': 393,\n",
              " 'igh': 394,\n",
              " 'est': 395,\n",
              " 'ist': 396,\n",
              " 'ab': 397,\n",
              " 'rom': 398,\n",
              " 'ƒ†N': 399,\n",
              " 'th': 400,\n",
              " 'ƒ†com': 401,\n",
              " 'ƒ†G': 402,\n",
              " 'un': 403,\n",
              " 'op': 404,\n",
              " '00': 405,\n",
              " 'ƒ†L': 406,\n",
              " 'ƒ†not': 407,\n",
              " 'ess': 408,\n",
              " 'ƒ†ex': 409,\n",
              " 'ƒ†v': 410,\n",
              " 'res': 411,\n",
              " 'ƒ†E': 412,\n",
              " 'ew': 413,\n",
              " 'ity': 414,\n",
              " 'ant': 415,\n",
              " 'ƒ†by': 416,\n",
              " 'el': 417,\n",
              " 'os': 418,\n",
              " 'ort': 419,\n",
              " 'oc': 420,\n",
              " 'qu': 421,\n",
              " 'ƒ†from': 422,\n",
              " 'ƒ†have': 423,\n",
              " 'ƒ†su': 424,\n",
              " 'ive': 425,\n",
              " 'ould': 426,\n",
              " 'ƒ†sh': 427,\n",
              " 'ƒ†this': 428,\n",
              " 'nt': 429,\n",
              " 'ra': 430,\n",
              " 'pe': 431,\n",
              " 'ight': 432,\n",
              " 'art': 433,\n",
              " 'ment': 434,\n",
              " 'ƒ†al': 435,\n",
              " 'ust': 436,\n",
              " 'end': 437,\n",
              " '--': 438,\n",
              " 'all': 439,\n",
              " 'ƒ†O': 440,\n",
              " 'ack': 441,\n",
              " 'ƒ†ch': 442,\n",
              " 'ƒ†le': 443,\n",
              " 'ies': 444,\n",
              " 'red': 445,\n",
              " 'ard': 446,\n",
              " '√¢ƒ¢': 447,\n",
              " 'out': 448,\n",
              " 'ƒ†J': 449,\n",
              " 'ƒ†ab': 450,\n",
              " 'ear': 451,\n",
              " 'iv': 452,\n",
              " 'ally': 453,\n",
              " 'our': 454,\n",
              " 'ost': 455,\n",
              " 'gh': 456,\n",
              " 'pt': 457,\n",
              " 'ƒ†pl': 458,\n",
              " 'ast': 459,\n",
              " 'ƒ†can': 460,\n",
              " 'ak': 461,\n",
              " 'ome': 462,\n",
              " 'ud': 463,\n",
              " 'The': 464,\n",
              " 'ƒ†his': 465,\n",
              " 'ƒ†do': 466,\n",
              " 'ƒ†go': 467,\n",
              " 'ƒ†has': 468,\n",
              " 'ge': 469,\n",
              " \"'t\": 470,\n",
              " 'ƒ†U': 471,\n",
              " 'rou': 472,\n",
              " 'ƒ†sa': 473,\n",
              " 'ƒ†j': 474,\n",
              " 'ƒ†but': 475,\n",
              " 'ƒ†wor': 476,\n",
              " 'ƒ†all': 477,\n",
              " 'ect': 478,\n",
              " 'ƒ†k': 479,\n",
              " 'ame': 480,\n",
              " 'ƒ†will': 481,\n",
              " 'ok': 482,\n",
              " 'ƒ†whe': 483,\n",
              " 'ƒ†they': 484,\n",
              " 'ide': 485,\n",
              " '01': 486,\n",
              " 'ff': 487,\n",
              " 'ich': 488,\n",
              " 'pl': 489,\n",
              " 'ther': 490,\n",
              " 'ƒ†tr': 491,\n",
              " '..': 492,\n",
              " 'ƒ†int': 493,\n",
              " 'ie': 494,\n",
              " 'ure': 495,\n",
              " 'age': 496,\n",
              " 'ƒ†ne': 497,\n",
              " 'ial': 498,\n",
              " 'ap': 499,\n",
              " 'ine': 500,\n",
              " 'ice': 501,\n",
              " 'ƒ†me': 502,\n",
              " 'ƒ†out': 503,\n",
              " 'ans': 504,\n",
              " 'one': 505,\n",
              " 'ong': 506,\n",
              " 'ions': 507,\n",
              " 'ƒ†who': 508,\n",
              " 'ƒ†K': 509,\n",
              " 'ƒ†up': 510,\n",
              " 'ƒ†their': 511,\n",
              " 'ƒ†ad': 512,\n",
              " 'ƒ†3': 513,\n",
              " 'ƒ†us': 514,\n",
              " 'ated': 515,\n",
              " 'ous': 516,\n",
              " 'ƒ†more': 517,\n",
              " 'ue': 518,\n",
              " 'og': 519,\n",
              " 'ƒ†St': 520,\n",
              " 'ind': 521,\n",
              " 'ike': 522,\n",
              " 'ƒ†so': 523,\n",
              " 'ime': 524,\n",
              " 'per': 525,\n",
              " '.\"': 526,\n",
              " 'ber': 527,\n",
              " 'iz': 528,\n",
              " 'act': 529,\n",
              " 'ƒ†one': 530,\n",
              " 'ƒ†said': 531,\n",
              " 'ƒ†-': 532,\n",
              " 'are': 533,\n",
              " 'ƒ†your': 534,\n",
              " 'cc': 535,\n",
              " 'ƒ†Th': 536,\n",
              " 'ƒ†cl': 537,\n",
              " 'ep': 538,\n",
              " 'ake': 539,\n",
              " 'able': 540,\n",
              " 'ip': 541,\n",
              " 'ƒ†cont': 542,\n",
              " 'ƒ†which': 543,\n",
              " 'ia': 544,\n",
              " 'ƒ†im': 545,\n",
              " 'ƒ†about': 546,\n",
              " 'ƒ†were': 547,\n",
              " 'very': 548,\n",
              " 'ub': 549,\n",
              " 'ƒ†had': 550,\n",
              " 'ƒ†en': 551,\n",
              " 'ƒ†comp': 552,\n",
              " ',\"': 553,\n",
              " 'ƒ†In': 554,\n",
              " 'ƒ†un': 555,\n",
              " 'ƒ†ag': 556,\n",
              " 'ire': 557,\n",
              " 'ace': 558,\n",
              " 'au': 559,\n",
              " 'ary': 560,\n",
              " 'ƒ†would': 561,\n",
              " 'ass': 562,\n",
              " 'ry': 563,\n",
              " 'ƒ†√¢ƒ¢': 564,\n",
              " 'cl': 565,\n",
              " 'ook': 566,\n",
              " 'ere': 567,\n",
              " 'so': 568,\n",
              " 'ƒ†V': 569,\n",
              " 'ign': 570,\n",
              " 'ib': 571,\n",
              " 'ƒ†off': 572,\n",
              " 'ƒ†te': 573,\n",
              " 'ven': 574,\n",
              " 'ƒ†Y': 575,\n",
              " 'ile': 576,\n",
              " 'ose': 577,\n",
              " 'ite': 578,\n",
              " 'orm': 579,\n",
              " 'ƒ†201': 580,\n",
              " 'ƒ†res': 581,\n",
              " 'ƒ†man': 582,\n",
              " 'ƒ†per': 583,\n",
              " 'ƒ†other': 584,\n",
              " 'ord': 585,\n",
              " 'ult': 586,\n",
              " 'ƒ†been': 587,\n",
              " 'ƒ†like': 588,\n",
              " 'ase': 589,\n",
              " 'ance': 590,\n",
              " 'ks': 591,\n",
              " 'ays': 592,\n",
              " 'own': 593,\n",
              " 'ence': 594,\n",
              " 'ƒ†dis': 595,\n",
              " 'ction': 596,\n",
              " 'ƒ†any': 597,\n",
              " 'ƒ†app': 598,\n",
              " 'ƒ†sp': 599,\n",
              " 'int': 600,\n",
              " 'ress': 601,\n",
              " 'ations': 602,\n",
              " 'ail': 603,\n",
              " 'ƒ†4': 604,\n",
              " 'ical': 605,\n",
              " 'ƒ†them': 606,\n",
              " 'ƒ†her': 607,\n",
              " 'ount': 608,\n",
              " 'ƒ†Ch': 609,\n",
              " 'ƒ†ar': 610,\n",
              " 'ƒ†if': 611,\n",
              " 'ƒ†there': 612,\n",
              " 'ƒ†pe': 613,\n",
              " 'ƒ†year': 614,\n",
              " 'av': 615,\n",
              " 'ƒ†my': 616,\n",
              " 'ƒ†some': 617,\n",
              " 'ƒ†when': 618,\n",
              " 'ough': 619,\n",
              " 'ach': 620,\n",
              " 'ƒ†than': 621,\n",
              " 'ru': 622,\n",
              " 'ond': 623,\n",
              " 'ick': 624,\n",
              " 'ƒ†over': 625,\n",
              " 'vel': 626,\n",
              " 'ƒ†qu': 627,\n",
              " 'ƒäƒä': 628,\n",
              " 'ƒ†sc': 629,\n",
              " 'reat': 630,\n",
              " 'ree': 631,\n",
              " 'ƒ†It': 632,\n",
              " 'ound': 633,\n",
              " 'port': 634,\n",
              " 'ƒ†also': 635,\n",
              " 'ƒ†part': 636,\n",
              " 'fter': 637,\n",
              " 'ƒ†kn': 638,\n",
              " 'ƒ†bec': 639,\n",
              " 'ƒ†time': 640,\n",
              " 'ens': 641,\n",
              " 'ƒ†5': 642,\n",
              " 'ople': 643,\n",
              " 'ƒ†what': 644,\n",
              " 'ƒ†no': 645,\n",
              " 'du': 646,\n",
              " 'mer': 647,\n",
              " 'ang': 648,\n",
              " 'ƒ†new': 649,\n",
              " '----': 650,\n",
              " 'ƒ†get': 651,\n",
              " 'ory': 652,\n",
              " 'ition': 653,\n",
              " 'ings': 654,\n",
              " 'ƒ†just': 655,\n",
              " 'ƒ†into': 656,\n",
              " 'ƒ†0': 657,\n",
              " 'ents': 658,\n",
              " 'ove': 659,\n",
              " 'te': 660,\n",
              " 'ƒ†people': 661,\n",
              " 'ƒ†pre': 662,\n",
              " 'ƒ†its': 663,\n",
              " 'ƒ†rec': 664,\n",
              " 'ƒ†tw': 665,\n",
              " 'ian': 666,\n",
              " 'irst': 667,\n",
              " 'ark': 668,\n",
              " 'ors': 669,\n",
              " 'ƒ†work': 670,\n",
              " 'ade': 671,\n",
              " 'ob': 672,\n",
              " 'ƒ†she': 673,\n",
              " 'ƒ†our': 674,\n",
              " 'wn': 675,\n",
              " 'ink': 676,\n",
              " 'lic': 677,\n",
              " 'ƒ†19': 678,\n",
              " 'ƒ†He': 679,\n",
              " 'ish': 680,\n",
              " 'nder': 681,\n",
              " 'ause': 682,\n",
              " 'ƒ†him': 683,\n",
              " 'ons': 684,\n",
              " 'ƒ†[': 685,\n",
              " 'ƒ†ro': 686,\n",
              " 'form': 687,\n",
              " 'ild': 688,\n",
              " 'ates': 689,\n",
              " 'vers': 690,\n",
              " 'ƒ†only': 691,\n",
              " 'oll': 692,\n",
              " 'ƒ†spe': 693,\n",
              " 'ck': 694,\n",
              " 'ell': 695,\n",
              " 'amp': 696,\n",
              " 'ƒ†acc': 697,\n",
              " 'ƒ†bl': 698,\n",
              " 'ious': 699,\n",
              " 'urn': 700,\n",
              " 'ft': 701,\n",
              " 'ood': 702,\n",
              " 'ƒ†how': 703,\n",
              " 'hed': 704,\n",
              " \"ƒ†'\": 705,\n",
              " 'ƒ†after': 706,\n",
              " 'aw': 707,\n",
              " 'ƒ†att': 708,\n",
              " 'ov': 709,\n",
              " 'ne': 710,\n",
              " 'ƒ†play': 711,\n",
              " 'erv': 712,\n",
              " 'ict': 713,\n",
              " 'ƒ†could': 714,\n",
              " 'itt': 715,\n",
              " 'ƒ†am': 716,\n",
              " 'ƒ†first': 717,\n",
              " 'ƒ†6': 718,\n",
              " 'ƒ†act': 719,\n",
              " 'ƒ†$': 720,\n",
              " 'ec': 721,\n",
              " 'hing': 722,\n",
              " 'ual': 723,\n",
              " 'ull': 724,\n",
              " 'ƒ†comm': 725,\n",
              " 'oy': 726,\n",
              " 'old': 727,\n",
              " 'ces': 728,\n",
              " 'ater': 729,\n",
              " 'ƒ†fe': 730,\n",
              " 'ƒ†bet': 731,\n",
              " 'we': 732,\n",
              " 'iff': 733,\n",
              " 'ƒ†two': 734,\n",
              " 'ock': 735,\n",
              " 'ƒ†back': 736,\n",
              " ').': 737,\n",
              " 'ident': 738,\n",
              " 'ƒ†under': 739,\n",
              " 'rough': 740,\n",
              " 'sel': 741,\n",
              " 'xt': 742,\n",
              " 'ƒ†may': 743,\n",
              " 'round': 744,\n",
              " 'ƒ†po': 745,\n",
              " 'ph': 746,\n",
              " 'iss': 747,\n",
              " 'ƒ†des': 748,\n",
              " 'ƒ†most': 749,\n",
              " 'ƒ†did': 750,\n",
              " 'ƒ†add': 751,\n",
              " 'ject': 752,\n",
              " 'ƒ†inc': 753,\n",
              " 'fore': 754,\n",
              " 'ƒ†pol': 755,\n",
              " 'ont': 756,\n",
              " 'ƒ†again': 757,\n",
              " 'clud': 758,\n",
              " 'tern': 759,\n",
              " 'ƒ†know': 760,\n",
              " 'ƒ†need': 761,\n",
              " 'ƒ†cons': 762,\n",
              " 'ƒ†co': 763,\n",
              " 'ƒ†.': 764,\n",
              " 'ƒ†want': 765,\n",
              " 'ƒ†see': 766,\n",
              " 'ƒ†7': 767,\n",
              " 'ning': 768,\n",
              " 'iew': 769,\n",
              " 'ƒ†This': 770,\n",
              " 'ced': 771,\n",
              " 'ƒ†even': 772,\n",
              " 'ƒ†ind': 773,\n",
              " 'ty': 774,\n",
              " 'ƒ†We': 775,\n",
              " 'ath': 776,\n",
              " 'ƒ†these': 777,\n",
              " 'ƒ†pr': 778,\n",
              " 'ƒ†use': 779,\n",
              " 'ƒ†because': 780,\n",
              " 'ƒ†fl': 781,\n",
              " 'ng': 782,\n",
              " 'ƒ†now': 783,\n",
              " 'ƒ†√¢ƒ¢ƒµ': 784,\n",
              " 'com': 785,\n",
              " 'ise': 786,\n",
              " 'ƒ†make': 787,\n",
              " 'ƒ†then': 788,\n",
              " 'ower': 789,\n",
              " 'ƒ†every': 790,\n",
              " 'ƒ†Un': 791,\n",
              " 'ƒ†sec': 792,\n",
              " 'oss': 793,\n",
              " 'uch': 794,\n",
              " 'ƒ†em': 795,\n",
              " 'ƒ†=': 796,\n",
              " 'ƒ†Re': 797,\n",
              " 'ied': 798,\n",
              " 'rit': 799,\n",
              " 'ƒ†inv': 800,\n",
              " 'lect': 801,\n",
              " 'ƒ†supp': 802,\n",
              " 'ating': 803,\n",
              " 'ƒ†look': 804,\n",
              " 'man': 805,\n",
              " 'pect': 806,\n",
              " 'ƒ†8': 807,\n",
              " 'row': 808,\n",
              " 'ƒ†bu': 809,\n",
              " 'ƒ†where': 810,\n",
              " 'ific': 811,\n",
              " 'ƒ†years': 812,\n",
              " 'ily': 813,\n",
              " 'ƒ†diff': 814,\n",
              " 'ƒ†should': 815,\n",
              " 'ƒ†rem': 816,\n",
              " 'Th': 817,\n",
              " 'In': 818,\n",
              " 'ƒ†ev': 819,\n",
              " 'day': 820,\n",
              " \"'re\": 821,\n",
              " 'rib': 822,\n",
              " 'ƒ†rel': 823,\n",
              " 'ss': 824,\n",
              " 'ƒ†def': 825,\n",
              " 'ƒ†right': 826,\n",
              " 'ƒ†sy': 827,\n",
              " '),': 828,\n",
              " 'les': 829,\n",
              " '000': 830,\n",
              " 'hen': 831,\n",
              " 'ƒ†through': 832,\n",
              " 'ƒ†Tr': 833,\n",
              " '__': 834,\n",
              " 'ƒ†way': 835,\n",
              " 'ƒ†don': 836,\n",
              " 'ƒ†,': 837,\n",
              " 'ƒ†10': 838,\n",
              " 'ased': 839,\n",
              " 'ƒ†ass': 840,\n",
              " 'ublic': 841,\n",
              " 'ƒ†reg': 842,\n",
              " 'ƒ†And': 843,\n",
              " 'ix': 844,\n",
              " 'ƒ†very': 845,\n",
              " 'ƒ†includ': 846,\n",
              " 'other': 847,\n",
              " 'ƒ†imp': 848,\n",
              " 'oth': 849,\n",
              " 'ƒ†sub': 850,\n",
              " 'ƒ†√¢ƒ¢ƒ∂': 851,\n",
              " 'ƒ†being': 852,\n",
              " 'arg': 853,\n",
              " 'ƒ†Wh': 854,\n",
              " '==': 855,\n",
              " 'ible': 856,\n",
              " 'ƒ†does': 857,\n",
              " 'ange': 858,\n",
              " 'ram': 859,\n",
              " 'ƒ†9': 860,\n",
              " 'ert': 861,\n",
              " 'ps': 862,\n",
              " 'ited': 863,\n",
              " 'ational': 864,\n",
              " 'ƒ†br': 865,\n",
              " 'ƒ†down': 866,\n",
              " 'ƒ†many': 867,\n",
              " 'aking': 868,\n",
              " 'ƒ†call': 869,\n",
              " 'uring': 870,\n",
              " 'ities': 871,\n",
              " 'ƒ†ph': 872,\n",
              " 'ics': 873,\n",
              " 'als': 874,\n",
              " 'ƒ†dec': 875,\n",
              " 'ative': 876,\n",
              " 'ener': 877,\n",
              " 'ƒ†before': 878,\n",
              " 'ility': 879,\n",
              " 'ƒ†well': 880,\n",
              " 'ƒ†much': 881,\n",
              " 'erson': 882,\n",
              " 'ƒ†those': 883,\n",
              " 'ƒ†such': 884,\n",
              " 'ƒ†ke': 885,\n",
              " 'ƒ†end': 886,\n",
              " 'ƒ†But': 887,\n",
              " 'ason': 888,\n",
              " 'ting': 889,\n",
              " 'ƒ†long': 890,\n",
              " 'ef': 891,\n",
              " 'ƒ†think': 892,\n",
              " 'ys': 893,\n",
              " 'ƒ†bel': 894,\n",
              " 'ƒ†sm': 895,\n",
              " 'its': 896,\n",
              " 'ax': 897,\n",
              " 'ƒ†own': 898,\n",
              " 'ƒ†prov': 899,\n",
              " 'ƒ†set': 900,\n",
              " 'ife': 901,\n",
              " 'ments': 902,\n",
              " 'ble': 903,\n",
              " 'ward': 904,\n",
              " 'ƒ†show': 905,\n",
              " 'ƒ†pres': 906,\n",
              " 'ms': 907,\n",
              " 'omet': 908,\n",
              " 'ƒ†ob': 909,\n",
              " 'ƒ†say': 910,\n",
              " 'ƒ†Sh': 911,\n",
              " 'ts': 912,\n",
              " 'ful': 913,\n",
              " 'ƒ†eff': 914,\n",
              " 'ƒ†gu': 915,\n",
              " 'ƒ†inst': 916,\n",
              " 'und': 917,\n",
              " 'ren': 918,\n",
              " 'cess': 919,\n",
              " 'ƒ†ent': 920,\n",
              " 'ƒ†You': 921,\n",
              " 'ƒ†good': 922,\n",
              " 'ƒ†start': 923,\n",
              " 'ince': 924,\n",
              " 'ƒ†made': 925,\n",
              " 'tt': 926,\n",
              " 'stem': 927,\n",
              " 'olog': 928,\n",
              " 'up': 929,\n",
              " 'ƒ†|': 930,\n",
              " 'ump': 931,\n",
              " 'ƒ†hel': 932,\n",
              " 'vern': 933,\n",
              " 'ular': 934,\n",
              " 'ually': 935,\n",
              " 'ƒ†ac': 936,\n",
              " 'ƒ†mon': 937,\n",
              " 'ƒ†last': 938,\n",
              " 'ƒ†200': 939,\n",
              " '10': 940,\n",
              " 'ƒ†stud': 941,\n",
              " 'ures': 942,\n",
              " 'ƒ†Ar': 943,\n",
              " 'self': 944,\n",
              " 'ars': 945,\n",
              " 'meric': 946,\n",
              " 'ues': 947,\n",
              " 'cy': 948,\n",
              " 'ƒ†min': 949,\n",
              " 'ollow': 950,\n",
              " 'ƒ†col': 951,\n",
              " 'io': 952,\n",
              " 'ƒ†mod': 953,\n",
              " 'ƒ†count': 954,\n",
              " 'ƒ†Com': 955,\n",
              " 'hes': 956,\n",
              " 'ƒ†fin': 957,\n",
              " 'air': 958,\n",
              " 'ier': 959,\n",
              " '√¢ƒ¢ƒ∂': 960,\n",
              " 'read': 961,\n",
              " 'ank': 962,\n",
              " 'atch': 963,\n",
              " 'ever': 964,\n",
              " 'ƒ†str': 965,\n",
              " 'ƒ†point': 966,\n",
              " 'ork': 967,\n",
              " 'ƒ†New': 968,\n",
              " 'ƒ†sur': 969,\n",
              " 'ool': 970,\n",
              " 'alk': 971,\n",
              " 'ement': 972,\n",
              " 'ƒ†used': 973,\n",
              " 'ract': 974,\n",
              " 'ween': 975,\n",
              " 'ƒ†same': 976,\n",
              " 'oun': 977,\n",
              " 'ƒ†Al': 978,\n",
              " 'ci': 979,\n",
              " 'ƒ†differe': 980,\n",
              " 'ƒ†while': 981,\n",
              " '--------': 982,\n",
              " 'ƒ†game': 983,\n",
              " 'cept': 984,\n",
              " 'ƒ†sim': 985,\n",
              " '...': 986,\n",
              " 'ƒ†inter': 987,\n",
              " 'ek': 988,\n",
              " 'ƒ†report': 989,\n",
              " 'ƒ†produ': 990,\n",
              " 'ƒ†still': 991,\n",
              " 'led': 992,\n",
              " 'ah': 993,\n",
              " 'ƒ†here': 994,\n",
              " 'ƒ†world': 995,\n",
              " 'ƒ†though': 996,\n",
              " 'ƒ†num': 997,\n",
              " 'arch': 998,\n",
              " 'imes': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#handling speacial token allowed_special\n",
        "#u can fork cl100k_base and add a new special token"
      ],
      "metadata": {
        "id": "pGO9bsJvo7w4"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**minbpe**\n",
        "\n",
        "minbpe/exercice.py"
      ],
      "metadata": {
        "id": "pxWD2YizqPYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TikToken tokenizer library used by GPT-3.5, GPT-4, GPT-4o, GPT-4.1, GPT-5-mini\n",
        "#It is:Byte-Level BPE with many improvements\n",
        "import tiktoken\n",
        "#enc = tiktoken.get_encoding(\"gpt2\")\n",
        "enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "ids = enc.encode(\"Hello world!\")\n",
        "print(ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDlEam8_pfhg",
        "outputId": "7e7d8d97-9d88-409e-cd87-9adbf6ce7f85"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13225, 2375, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(enc.decode(ids))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sds-ykLVrcJq",
        "outputId": "47bc410a-b354-4faf-94d2-9b9cdabe1279"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SentencePiece Tokenizer language-independent subword tokenizer.\n",
        "# used for both training and inference\n",
        "import sentencepiece as spm\n",
        "\n",
        "\n",
        "#In th Tik token we first take our code points in the string and\n",
        "#we encode them used utf-8 to bytes and we're merging bytes\n",
        "\n",
        "# vs\n",
        "\n",
        "#for SentencePiece it works directly on the level of the code points themselves so\n",
        "# it looks at whatever code points are available in the training set\n",
        "# and start merging those code points and the BPE is running on the level of code points\n",
        "#git mapped to a special unkown token or if u have a bite foldback option turned on"
      ],
      "metadata": {
        "id": "3X6r6EfrrkBp"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  SentencePiece ‚Äî Code Point‚ÄìLevel BPE\n",
        "\n",
        "SentencePiece est un tokenizer BPE largement utilis√© car **il peut √† la fois s‚Äôentra√Æner et servir √† l‚Äôinf√©rence**, contrairement √† TikToken (qui n‚Äôest pas con√ßu pour l‚Äôentra√Ænement).\n",
        "\n",
        "C‚Äôest le tokenizer officiel de nombreux mod√®les modernes :\n",
        "- **LLaMA**  ,**Mistral** , **T5** , **ALBERT**  et d‚Äôautres\n",
        "\n",
        " **Repo officiel** : https://github.com/google/sentencepiece\n",
        "\n",
        "---\n",
        "\n",
        "## Fonctionnement : BPE sur les Points de Code Unicode\n",
        "\n",
        "SentencePiece applique le BPE **directement sur les points de code Unicode**, et non sur les bytes UTF-8.  \n",
        "Cela lui permet de travailler proprement sur toutes les langues, y compris celles **sans espaces** (comme le japonais ou le chinois).\n",
        "\n",
        "---\n",
        "\n",
        "##  Hyperparam√®tre : `character_coverage`\n",
        "\n",
        "`character_coverage` d√©termine quels caract√®res doivent √™tre int√©gr√©s au vocabulaire :\n",
        "\n",
        "- Si un caract√®re est **fr√©quent**, il fait partie du vocab.\n",
        "- Si un caract√®re est **rare** Unknown characters , deux cas :\n",
        "\n",
        "###  1. Sans `byte_fallback`\n",
        "‚Üí Le caract√®re est remplac√© par le token **`<unk>`** (unknown).\n",
        "\n",
        "### 2. Avec `byte_fallback = true`\n",
        "‚Üí Le caract√®re rare est encod√© en **UTF-8 bytes**, puis converti en **tokens byte**.\n",
        "\n",
        "Cela garantit que tout caract√®re peut √™tre encod√©, m√™me s'il est absent du vocabulaire.\n",
        "\n",
        "---\n",
        "\n",
        "##  Propri√©t√©s importantes\n",
        "\n",
        "- Peut **fusionner des sous-mots** dans n‚Äôimporte quelle langue\n",
        "- Fonctionne dans les langues **sans espaces**\n",
        "- Produit g√©n√©ralement des **s√©quences plus courtes** que les tokenizers byte-level\n",
        "- N√©cessite une gestion explicite des caract√®res inconnus :\n",
        "  - soit `<unk>`\n",
        "  - soit byte fallback\n",
        "\n"
      ],
      "metadata": {
        "id": "oWYy9tZhtvfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#text = \"caf√©\"\n",
        "# TikToken (byte-level)\n",
        "# UTF-8 bytes: b'c' b'a' b'f' b'\\xc3' b'\\xa9'\n",
        "# BPE merges are applied on bytes\n",
        "# Could produce: ['c', 'a', 'f', '√©'] if merged or still byte tokens\n",
        "\n",
        "# SentencePiece (code point-level)\n",
        "# Code points: ['c', 'a', 'f', '√©'] (U+0063, U+0061, U+0066, U+00E9)\n",
        "# BPE merges applied directly on code points\n",
        "# Could produce: ['caf', '√©']\n",
        "# Llama2 use SentencePiece"
      ],
      "metadata": {
        "id": "mms8e8zStvIO"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#in LM we want to keep the raw data as much as possible in a raw form\n",
        "#si pour cela ils critiquent SentencePiece (normalisation)\n"
      ],
      "metadata": {
        "id": "VLWZezRGewQe"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "with open(\"toy.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "  f.write(\"SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\")"
      ],
      "metadata": {
        "id": "o7ZV7zSmkWJL"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train a sentencepiece model on it\n",
        "# the settings here are (best effort) those used for training Llama 2\n",
        "import os\n",
        "\n",
        "options = dict(\n",
        "  # input spec\n",
        "  input=\"toy.txt\",\n",
        "  input_format=\"text\",\n",
        "  # output spec\n",
        "  model_prefix=\"tok400\", # output filename prefix\n",
        "  # algorithm spec\n",
        "  # BPE alg\n",
        "  model_type=\"bpe\",\n",
        "  vocab_size=400,\n",
        "  # normalization\n",
        "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
        "  remove_extra_whitespaces=False,\n",
        "  input_sentence_size=200000000, # max number of training sentences\n",
        "  max_sentence_length=4192, # max number of bytes per sentence\n",
        "  seed_sentencepiece_size=1000000,\n",
        "  shuffle_input_sentence=True,\n",
        "  # rare word treatment\n",
        "  character_coverage=0.99995,\n",
        "  byte_fallback=True,\n",
        "  # merge rules\n",
        "  split_digits=True,\n",
        "  split_by_unicode_script=True,\n",
        "  split_by_whitespace=True,\n",
        "  split_by_number=True,\n",
        "  max_sentencepiece_length=16,\n",
        "  add_dummy_prefix=True,\n",
        "  allow_whitespace_only_pieces=True,\n",
        "  # special tokens\n",
        "  unk_id=0, # the UNK token MUST exist\n",
        "  bos_id=1, # the others are optional, set to -1 to turn off\n",
        "  eos_id=2,\n",
        "  pad_id=-1,\n",
        "  # systems\n",
        "  num_threads=os.cpu_count(), # use ~all system resources\n",
        ")\n",
        "\n",
        "spm.SentencePieceTrainer.train(**options)\n"
      ],
      "metadata": {
        "id": "6Y6W6kqLkjU0"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('tok400.model')\n",
        "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrGtYA3Akp1h",
        "outputId": "e3b32d43-00f2-42a3-b642-c426014394d9"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<unk>', 0],\n",
              " ['<s>', 1],\n",
              " ['</s>', 2],\n",
              " ['<0x00>', 3],\n",
              " ['<0x01>', 4],\n",
              " ['<0x02>', 5],\n",
              " ['<0x03>', 6],\n",
              " ['<0x04>', 7],\n",
              " ['<0x05>', 8],\n",
              " ['<0x06>', 9],\n",
              " ['<0x07>', 10],\n",
              " ['<0x08>', 11],\n",
              " ['<0x09>', 12],\n",
              " ['<0x0A>', 13],\n",
              " ['<0x0B>', 14],\n",
              " ['<0x0C>', 15],\n",
              " ['<0x0D>', 16],\n",
              " ['<0x0E>', 17],\n",
              " ['<0x0F>', 18],\n",
              " ['<0x10>', 19],\n",
              " ['<0x11>', 20],\n",
              " ['<0x12>', 21],\n",
              " ['<0x13>', 22],\n",
              " ['<0x14>', 23],\n",
              " ['<0x15>', 24],\n",
              " ['<0x16>', 25],\n",
              " ['<0x17>', 26],\n",
              " ['<0x18>', 27],\n",
              " ['<0x19>', 28],\n",
              " ['<0x1A>', 29],\n",
              " ['<0x1B>', 30],\n",
              " ['<0x1C>', 31],\n",
              " ['<0x1D>', 32],\n",
              " ['<0x1E>', 33],\n",
              " ['<0x1F>', 34],\n",
              " ['<0x20>', 35],\n",
              " ['<0x21>', 36],\n",
              " ['<0x22>', 37],\n",
              " ['<0x23>', 38],\n",
              " ['<0x24>', 39],\n",
              " ['<0x25>', 40],\n",
              " ['<0x26>', 41],\n",
              " ['<0x27>', 42],\n",
              " ['<0x28>', 43],\n",
              " ['<0x29>', 44],\n",
              " ['<0x2A>', 45],\n",
              " ['<0x2B>', 46],\n",
              " ['<0x2C>', 47],\n",
              " ['<0x2D>', 48],\n",
              " ['<0x2E>', 49],\n",
              " ['<0x2F>', 50],\n",
              " ['<0x30>', 51],\n",
              " ['<0x31>', 52],\n",
              " ['<0x32>', 53],\n",
              " ['<0x33>', 54],\n",
              " ['<0x34>', 55],\n",
              " ['<0x35>', 56],\n",
              " ['<0x36>', 57],\n",
              " ['<0x37>', 58],\n",
              " ['<0x38>', 59],\n",
              " ['<0x39>', 60],\n",
              " ['<0x3A>', 61],\n",
              " ['<0x3B>', 62],\n",
              " ['<0x3C>', 63],\n",
              " ['<0x3D>', 64],\n",
              " ['<0x3E>', 65],\n",
              " ['<0x3F>', 66],\n",
              " ['<0x40>', 67],\n",
              " ['<0x41>', 68],\n",
              " ['<0x42>', 69],\n",
              " ['<0x43>', 70],\n",
              " ['<0x44>', 71],\n",
              " ['<0x45>', 72],\n",
              " ['<0x46>', 73],\n",
              " ['<0x47>', 74],\n",
              " ['<0x48>', 75],\n",
              " ['<0x49>', 76],\n",
              " ['<0x4A>', 77],\n",
              " ['<0x4B>', 78],\n",
              " ['<0x4C>', 79],\n",
              " ['<0x4D>', 80],\n",
              " ['<0x4E>', 81],\n",
              " ['<0x4F>', 82],\n",
              " ['<0x50>', 83],\n",
              " ['<0x51>', 84],\n",
              " ['<0x52>', 85],\n",
              " ['<0x53>', 86],\n",
              " ['<0x54>', 87],\n",
              " ['<0x55>', 88],\n",
              " ['<0x56>', 89],\n",
              " ['<0x57>', 90],\n",
              " ['<0x58>', 91],\n",
              " ['<0x59>', 92],\n",
              " ['<0x5A>', 93],\n",
              " ['<0x5B>', 94],\n",
              " ['<0x5C>', 95],\n",
              " ['<0x5D>', 96],\n",
              " ['<0x5E>', 97],\n",
              " ['<0x5F>', 98],\n",
              " ['<0x60>', 99],\n",
              " ['<0x61>', 100],\n",
              " ['<0x62>', 101],\n",
              " ['<0x63>', 102],\n",
              " ['<0x64>', 103],\n",
              " ['<0x65>', 104],\n",
              " ['<0x66>', 105],\n",
              " ['<0x67>', 106],\n",
              " ['<0x68>', 107],\n",
              " ['<0x69>', 108],\n",
              " ['<0x6A>', 109],\n",
              " ['<0x6B>', 110],\n",
              " ['<0x6C>', 111],\n",
              " ['<0x6D>', 112],\n",
              " ['<0x6E>', 113],\n",
              " ['<0x6F>', 114],\n",
              " ['<0x70>', 115],\n",
              " ['<0x71>', 116],\n",
              " ['<0x72>', 117],\n",
              " ['<0x73>', 118],\n",
              " ['<0x74>', 119],\n",
              " ['<0x75>', 120],\n",
              " ['<0x76>', 121],\n",
              " ['<0x77>', 122],\n",
              " ['<0x78>', 123],\n",
              " ['<0x79>', 124],\n",
              " ['<0x7A>', 125],\n",
              " ['<0x7B>', 126],\n",
              " ['<0x7C>', 127],\n",
              " ['<0x7D>', 128],\n",
              " ['<0x7E>', 129],\n",
              " ['<0x7F>', 130],\n",
              " ['<0x80>', 131],\n",
              " ['<0x81>', 132],\n",
              " ['<0x82>', 133],\n",
              " ['<0x83>', 134],\n",
              " ['<0x84>', 135],\n",
              " ['<0x85>', 136],\n",
              " ['<0x86>', 137],\n",
              " ['<0x87>', 138],\n",
              " ['<0x88>', 139],\n",
              " ['<0x89>', 140],\n",
              " ['<0x8A>', 141],\n",
              " ['<0x8B>', 142],\n",
              " ['<0x8C>', 143],\n",
              " ['<0x8D>', 144],\n",
              " ['<0x8E>', 145],\n",
              " ['<0x8F>', 146],\n",
              " ['<0x90>', 147],\n",
              " ['<0x91>', 148],\n",
              " ['<0x92>', 149],\n",
              " ['<0x93>', 150],\n",
              " ['<0x94>', 151],\n",
              " ['<0x95>', 152],\n",
              " ['<0x96>', 153],\n",
              " ['<0x97>', 154],\n",
              " ['<0x98>', 155],\n",
              " ['<0x99>', 156],\n",
              " ['<0x9A>', 157],\n",
              " ['<0x9B>', 158],\n",
              " ['<0x9C>', 159],\n",
              " ['<0x9D>', 160],\n",
              " ['<0x9E>', 161],\n",
              " ['<0x9F>', 162],\n",
              " ['<0xA0>', 163],\n",
              " ['<0xA1>', 164],\n",
              " ['<0xA2>', 165],\n",
              " ['<0xA3>', 166],\n",
              " ['<0xA4>', 167],\n",
              " ['<0xA5>', 168],\n",
              " ['<0xA6>', 169],\n",
              " ['<0xA7>', 170],\n",
              " ['<0xA8>', 171],\n",
              " ['<0xA9>', 172],\n",
              " ['<0xAA>', 173],\n",
              " ['<0xAB>', 174],\n",
              " ['<0xAC>', 175],\n",
              " ['<0xAD>', 176],\n",
              " ['<0xAE>', 177],\n",
              " ['<0xAF>', 178],\n",
              " ['<0xB0>', 179],\n",
              " ['<0xB1>', 180],\n",
              " ['<0xB2>', 181],\n",
              " ['<0xB3>', 182],\n",
              " ['<0xB4>', 183],\n",
              " ['<0xB5>', 184],\n",
              " ['<0xB6>', 185],\n",
              " ['<0xB7>', 186],\n",
              " ['<0xB8>', 187],\n",
              " ['<0xB9>', 188],\n",
              " ['<0xBA>', 189],\n",
              " ['<0xBB>', 190],\n",
              " ['<0xBC>', 191],\n",
              " ['<0xBD>', 192],\n",
              " ['<0xBE>', 193],\n",
              " ['<0xBF>', 194],\n",
              " ['<0xC0>', 195],\n",
              " ['<0xC1>', 196],\n",
              " ['<0xC2>', 197],\n",
              " ['<0xC3>', 198],\n",
              " ['<0xC4>', 199],\n",
              " ['<0xC5>', 200],\n",
              " ['<0xC6>', 201],\n",
              " ['<0xC7>', 202],\n",
              " ['<0xC8>', 203],\n",
              " ['<0xC9>', 204],\n",
              " ['<0xCA>', 205],\n",
              " ['<0xCB>', 206],\n",
              " ['<0xCC>', 207],\n",
              " ['<0xCD>', 208],\n",
              " ['<0xCE>', 209],\n",
              " ['<0xCF>', 210],\n",
              " ['<0xD0>', 211],\n",
              " ['<0xD1>', 212],\n",
              " ['<0xD2>', 213],\n",
              " ['<0xD3>', 214],\n",
              " ['<0xD4>', 215],\n",
              " ['<0xD5>', 216],\n",
              " ['<0xD6>', 217],\n",
              " ['<0xD7>', 218],\n",
              " ['<0xD8>', 219],\n",
              " ['<0xD9>', 220],\n",
              " ['<0xDA>', 221],\n",
              " ['<0xDB>', 222],\n",
              " ['<0xDC>', 223],\n",
              " ['<0xDD>', 224],\n",
              " ['<0xDE>', 225],\n",
              " ['<0xDF>', 226],\n",
              " ['<0xE0>', 227],\n",
              " ['<0xE1>', 228],\n",
              " ['<0xE2>', 229],\n",
              " ['<0xE3>', 230],\n",
              " ['<0xE4>', 231],\n",
              " ['<0xE5>', 232],\n",
              " ['<0xE6>', 233],\n",
              " ['<0xE7>', 234],\n",
              " ['<0xE8>', 235],\n",
              " ['<0xE9>', 236],\n",
              " ['<0xEA>', 237],\n",
              " ['<0xEB>', 238],\n",
              " ['<0xEC>', 239],\n",
              " ['<0xED>', 240],\n",
              " ['<0xEE>', 241],\n",
              " ['<0xEF>', 242],\n",
              " ['<0xF0>', 243],\n",
              " ['<0xF1>', 244],\n",
              " ['<0xF2>', 245],\n",
              " ['<0xF3>', 246],\n",
              " ['<0xF4>', 247],\n",
              " ['<0xF5>', 248],\n",
              " ['<0xF6>', 249],\n",
              " ['<0xF7>', 250],\n",
              " ['<0xF8>', 251],\n",
              " ['<0xF9>', 252],\n",
              " ['<0xFA>', 253],\n",
              " ['<0xFB>', 254],\n",
              " ['<0xFC>', 255],\n",
              " ['<0xFD>', 256],\n",
              " ['<0xFE>', 257],\n",
              " ['<0xFF>', 258],\n",
              " ['en', 259],\n",
              " ['‚ñÅt', 260],\n",
              " ['ce', 261],\n",
              " ['in', 262],\n",
              " ['ra', 263],\n",
              " ['‚ñÅa', 264],\n",
              " ['de', 265],\n",
              " ['er', 266],\n",
              " ['‚ñÅs', 267],\n",
              " ['ent', 268],\n",
              " ['or', 269],\n",
              " ['pr', 270],\n",
              " ['‚ñÅm', 271],\n",
              " ['‚ñÅu', 272],\n",
              " ['ing', 273],\n",
              " ['‚ñÅth', 274],\n",
              " ['ence', 275],\n",
              " ['entence', 276],\n",
              " ['Pi', 277],\n",
              " ['ed', 278],\n",
              " ['em', 279],\n",
              " ['ex', 280],\n",
              " ['is', 281],\n",
              " ['iz', 282],\n",
              " ['la', 283],\n",
              " ['on', 284],\n",
              " ['st', 285],\n",
              " ['‚ñÅS', 286],\n",
              " ['Pie', 287],\n",
              " ['end', 288],\n",
              " ['ext', 289],\n",
              " ['‚ñÅan', 290],\n",
              " ['‚ñÅpr', 291],\n",
              " ['‚ñÅto', 292],\n",
              " ['‚ñÅun', 293],\n",
              " ['‚ñÅthe', 294],\n",
              " ['Piece', 295],\n",
              " ['‚ñÅSentence', 296],\n",
              " ['‚ñÅSentencePiece', 297],\n",
              " ['.]', 298],\n",
              " ['Ne', 299],\n",
              " ['ag', 300],\n",
              " ['do', 301],\n",
              " ['ec', 302],\n",
              " ['gu', 303],\n",
              " ['ic', 304],\n",
              " ['ir', 305],\n",
              " ['it', 306],\n",
              " ['ly', 307],\n",
              " ['to', 308],\n",
              " ['‚ñÅ(', 309],\n",
              " ['‚ñÅ[', 310],\n",
              " ['‚ñÅf', 311],\n",
              " ['‚ñÅn', 312],\n",
              " ['‚ñÅw', 313],\n",
              " ['.])', 314],\n",
              " ['age', 315],\n",
              " ['del', 316],\n",
              " ['ion', 317],\n",
              " ['ken', 318],\n",
              " ['lan', 319],\n",
              " ['ral', 320],\n",
              " ['wor', 321],\n",
              " ['yst', 322],\n",
              " ['‚ñÅNe', 323],\n",
              " ['‚ñÅal', 324],\n",
              " ['‚ñÅde', 325],\n",
              " ['‚ñÅis', 326],\n",
              " ['‚ñÅma', 327],\n",
              " ['‚ñÅmo', 328],\n",
              " ['izer', 329],\n",
              " ['rain', 330],\n",
              " ['ural', 331],\n",
              " ['‚ñÅand', 332],\n",
              " ['‚ñÅlan', 333],\n",
              " ['‚ñÅpre', 334],\n",
              " ['guage', 335],\n",
              " ['ystem', 336],\n",
              " ['‚ñÅtext', 337],\n",
              " ['‚ñÅmodel', 338],\n",
              " ['‚ñÅtrain', 339],\n",
              " ['kenizer', 340],\n",
              " ['‚ñÅsystem', 341],\n",
              " ['‚ñÅlanguage', 342],\n",
              " ['‚ñÅtraining', 343],\n",
              " ['.,', 344],\n",
              " ['BP', 345],\n",
              " ['Ku', 346],\n",
              " ['ab', 347],\n",
              " ['as', 348],\n",
              " ['at', 349],\n",
              " ['by', 350],\n",
              " ['co', 351],\n",
              " ['es', 352],\n",
              " ['et', 353],\n",
              " ['if', 354],\n",
              " ['ig', 355],\n",
              " ['im', 356],\n",
              " ['ke', 357],\n",
              " ['lo', 358],\n",
              " ['nr', 359],\n",
              " ['oc', 360],\n",
              " ['e', 361],\n",
              " ['‚ñÅ', 362],\n",
              " ['n', 363],\n",
              " ['t', 364],\n",
              " ['i', 365],\n",
              " ['r', 366],\n",
              " ['a', 367],\n",
              " ['o', 368],\n",
              " ['s', 369],\n",
              " ['d', 370],\n",
              " ['c', 371],\n",
              " ['l', 372],\n",
              " ['u', 373],\n",
              " ['g', 374],\n",
              " ['m', 375],\n",
              " ['p', 376],\n",
              " ['.', 377],\n",
              " ['h', 378],\n",
              " ['-', 379],\n",
              " ['w', 380],\n",
              " ['y', 381],\n",
              " ['P', 382],\n",
              " ['S', 383],\n",
              " ['b', 384],\n",
              " ['f', 385],\n",
              " ['k', 386],\n",
              " [')', 387],\n",
              " ['x', 388],\n",
              " ['z', 389],\n",
              " ['(', 390],\n",
              " ['N', 391],\n",
              " ['[', 392],\n",
              " [']', 393],\n",
              " ['v', 394],\n",
              " [',', 395],\n",
              " ['/', 396],\n",
              " ['B', 397],\n",
              " ['E', 398],\n",
              " ['K', 399]]"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = sp.encode(\"hello ÏïàÎÖïÌïòÏÑ∏Ïöî  ŸÖÿ±ÿ≠ÿ®ÿß\")\n",
        "print(ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcY6USK6ku7E",
        "outputId": "56799d36-dab4-4802-a1a2-eae071ddb243"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[362, 378, 361, 372, 358, 362, 239, 152, 139, 238, 136, 152, 240, 152, 155, 239, 135, 187, 239, 157, 151, 362, 362, 220, 136, 219, 180, 219, 176, 219, 171, 219, 170]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#decoding indiv tokens back into the little pieces\n",
        "print([sp.id_to_piece(idx) for idx in ids])\n",
        "#bytecode of unkown token\n",
        "#qui n'appartiennent pas a notre training set\n",
        "#  byte_fallback=False,\n",
        "# we gonna get <unk> in the decoding\n",
        "#llama use true\n",
        "#white space = '_' and extra space in the begining because of   add_dummy_prefix=True,\n",
        "# 'word' and  ' word' are different so we add spaces in begin\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt0SeWl0k4C6",
        "outputId": "ffd93852-3632-43c3-c0b0-2232515f9f92"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['‚ñÅ', 'h', 'e', 'l', 'lo', '‚ñÅ', '<0xEC>', '<0x95>', '<0x88>', '<0xEB>', '<0x85>', '<0x95>', '<0xED>', '<0x95>', '<0x98>', '<0xEC>', '<0x84>', '<0xB8>', '<0xEC>', '<0x9A>', '<0x94>', '‚ñÅ', '‚ñÅ', '<0xD9>', '<0x85>', '<0xD8>', '<0xB1>', '<0xD8>', '<0xAD>', '<0xD8>', '<0xA8>', '<0xD8>', '<0xA7>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "==="
      ],
      "metadata": {
        "id": "TEJBrIqrnOnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "vocab_size\n",
        "\n",
        "Q: what should be vocab size?\n",
        "\n",
        "Q: how can I increase vocab size?\n",
        "\n",
        "dans le code precedent ou on code mini_gpt from scratch on a self.\n",
        "\n",
        "code transformeur\n",
        "\n",
        "```\n",
        "token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "\n",
        "```\n",
        "table d'emb 2_dim array vocab_size number of rows\n",
        "each token has a vector that we're going to train using back propagation\n",
        "\n",
        "the vector is of size n_emb= number of channels in the Transformer and basically as vocap size increases we're going to add rows in the table\n",
        "\n",
        "\n",
        "lm_head(x) = nn.Linear(n_embd, vocab_size)to produce the logits = proba of the next token in the sequence"
      ],
      "metadata": {
        "id": "YHOFLJA5nNfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "> Learning to Compress Prompts with Gist Tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "3wW-NWkPqTxe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Soft Prompt Distillation: gist tokens**\n",
        "\n",
        "y a une technique train the model by distillation so you are keeping the entire model frozen and you're only training the representations of *the new tokens* their embeddings and you're optimizing over the new tokens such taht the behavior of the language model is identical to the model that has a very long prompt\n",
        "\n",
        "\n",
        "so it's a compression technique of compressing that very long prompt into those few new gist tokens\n",
        "\n",
        "ni Laura ni finetuning you're training just token embeddings"
      ],
      "metadata": {
        "id": "9DpproOBqyDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n"
      ],
      "metadata": {
        "id": "7FXWgSqOs_Rl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "[Let's build the GPT Tokenizer](https://https://www.youtube.com/watch?v=zduSFxRajkE)\n"
      ],
      "metadata": {
        "id": "wASyPGfWumJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> TAMING TRANSFORMERS FOR HIGH-RESOLUTION IMAGE SYNTHESIS\n",
        "\n"
      ],
      "metadata": {
        "id": "VMr_Oz6YtCaA"
      }
    }
  ]
}